298笔记

### 1.2 Embedding词嵌入

#### 1.2.1词嵌入介绍

**将词语映射（转换）为实数向量（一串数字）**

#### 1.2.2.词嵌入(Word Embedding又叫词向量)方法:

##### 1. OneHot

他的值只有0和1

缺点:

- **One-Hot编码中所有词向量之间彼此正交**时，意味着任意两个词向量之间的点积为0,这意味着它们在向量空间中是相互独立的，**没有体现词与词之间的相似性**, 只能做到区分词
- **One-Hot词向量在词表稀疏且高维，计算费资源和时间；用PCA强行压缩（降维）是个简单实用的救急办法**

优点:

比如:词"A"的One-Hot编码为[1, 0, 0, 0, 0]，词"B"的One-Hot编码为[0, 1, 0, 0, 0]，它们的点积为：

1·0 + 0·1 + 0·0 + 0·0 + 0·0 = 0

- ##### 为什么特征向量要映射到欧式空间?

将离散特征通过OneHot编码映射到欧式空间（也就是我们熟悉的笛卡尔坐标系），主要是为了方便进行数学计算和比较。

"离散值创建一个独立的二进制特征"这句话的意思是，对于每一个离散值，我们都会创建一个新的二进制特征（即只有0和1两种取值的特征），用来表示该离散值是否存在。

**举个例子**，假设我们有一个离散特征“颜色”，取值可以是“红色”、“蓝色”和“绿色”。如果我们用OneHot编码来处理这个特征，那么我们会创建三个新的二进制特征：

将这些离散特征转换成了一组连续的数值特征, 如果我们用OneHot编码，红色、蓝色和绿色分别对应[1, 0, 0]、[0, 1, 0]和[0, 0, 1]，那么这些颜色在欧式空间中的距离就是一样的,这样，每个离散值就对应了欧式空间中的一个点。这样一来，我们就可以使用**欧式空间中的各种距离和相似度计算方法**来处理这些特征了

**举个例子**，假设我们有一个离散特征“颜色”，取值可以是“红色”、“蓝色”和“绿色”。直接用数字表示这些颜色（比如红色=1，蓝色=2，绿色=3）可能会导致计算机错误地认为红色和蓝色之间的距离比红色和绿色之间的距离小

好的，我们来用 **文本相似度** 这个最常见的例子来解释余弦相似度（Cosine Similarity）。

**欧式空间中的余弦相似度核心思想：**
余弦相似度衡量的是两个**向量在方向上的差异**，而不是它们的大小（长度）。它关注的是它们是否指向同一个方向。

- **值 = 1：** 两个向量方向**完全相同**（夹角为0度）。
- **值 = 0：** 两个向量**互相垂直**（夹角为90度），完全不相关。
- **值 = -1：** 两个向量方向**完全相反**（夹角为180度）。
- 值越接近1，表示两个向量的方向越相似。

**余弦相似度为什么在文本中常用？**
因为我们可以把文档（句子、段落等）表示成高维空间中的向量（例如，每个维度代表一个词的出现次数或重要性）。余弦相似度可以忽略文档的长度差异（比如一个长文档和一个短文档可能谈论同一个主题），只关注它们包含的词语及其权重的“方向”是否一致。

**例子：比较三个句子的相似度**

1. **句子A：** “我喜欢读书”
2. **句子B：** “他喜欢看电影”
3. **句子C：** “天空是蓝色的”

**步骤1：创建词表 & 向量表示**

- **列出所有不重复的词：** 从三个句子中找出所有不同的词：`[我, 喜欢, 读书, 他, 看, 电影, 天空, 是, 蓝色的]` (共9个词)。
- **将句子表示为向量：** 每个句子的向量长度等于词表大小（9维）。向量的每个位置对应一个词，数值表示该词在句子中出现的次数（词频，TF）。
  - **向量A (我喜欢读书):** `[1, 1, 1, 0, 0, 0, 0, 0, 0]` (对应词：我, 喜欢, 读书)
  - **向量B (他喜欢看电影):** `[0, 1, 0, 1, 1, 1, 0, 0, 0]` (对应词：喜欢, 他, 看, 电影)
  - **向量C (天空是蓝色的):** `[0, 0, 0, 0, 0, 0, 1, 1, 1]` (对应词：天空, 是, 蓝色的)

**步骤2：计算余弦相似度（公式）**

余弦相似度 (Cosine Similarity) = `(A • B) / (||A|| * ||B||)`

- **A • B:** 向量A和向量B的**点积** (Dot Product)。计算方式：对应维度相乘后求和。
- **||A||:** 向量A的**模长** (Magnitude / Euclidean Norm)。计算方式：向量所有元素的平方和开根号 `sqrt(sum(x_i²))`。
- **||B||:** 向量B的模长。

**计算 A 和 B 的相似度：**

1. **点积 (A • B):**
   `(1*0) + (1*1) + (1*0) + (0*1) + (0*1) + (0*1) + (0*0) + (0*0) + (0*0) = 0 + 1 + 0 + 0 + 0 + 0 + 0 + 0 + 0 = 1`
2. **模长 ||A||:**
   `sqrt(1² + 1² + 1² + 0² + 0² + 0² + 0² + 0² + 0²) = sqrt(1 + 1 + 1) = sqrt(3) ≈ 1.732`
3. **模长 ||B||:**
   `sqrt(0² + 1² + 0² + 1² + 1² + 1² + 0² + 0² + 0²) = sqrt(0 + 1 + 0 + 1 + 1 + 1) = sqrt(4) = 2`
4. **余弦相似度 (A, B):**
   `(1) / (1.732 * 2) ≈ 1 / 3.464 ≈ 0.289`
5. 这两个句子都包含“喜欢”，并且谈论的都是人的爱好（读书 vs 看电影）。虽然具体爱好不同，但主题类型相似，所以相似度是正的（>0），但不太高（远小于1）

同理AC, BC相似度都是0两个句子谈论的是完全不同的主题（爱好 vs 天气），没有任何共同的词汇，所以相似度为0。

**适用场景**：衡量文本相似度、文档检索、推荐系统中衡量用户/物品兴趣方向的相似性等。当关心模式或趋势是否一致，而不关心绝对数值大小时效果好。

**2.欧式空间中的欧几里得距离 (Euclidean Distance):**

**适用场景**：聚类（如K-Means）、K近邻（KNN）等需要计算实际空间距离的任务。当特征有明确物理意义且量纲一致（或已标准化）时效果好。



##### 2. Word2Vec

Distributed representation（**分布式**表示）是自然语言处理（NLP）中的核心概念，也是解决 One-Hot 编码缺陷的核心思路。它的本质是：**用一个低维、稠密且连续的向量表示一个词（或符号），且这个词的语义信息被“分布式”存储在这个向量的各个维度中**。

分布式表示不是人为设定的，而是**从数据中自动学习语义特征**：

1. **训练目标**：让出现在相似上下文中的词具有相似向量。

   - 例：模型发现“猫”和“狗”常出现在“宠物”“喂食”附近 → 让二者的**向量接近**。

2. **经典算法**：

   - `Word2Vec`：通过预测上下文（Skip-gram）或预测中心词（CBOW）学习。

     CBOW模型通常包括输入层、隐藏层和输出层。输入层接收上下文词的one-hot编码，隐藏层通过权重矩阵将输入转换为低维的密集向量，输出层则使用softmax函数来预测目标词的概率分布

     Skip-Gram模型同样包括输入层、隐藏层和输出层。但在这里，输入层只接收中心词的one-hot编码，隐藏层同样通过权重矩阵转换为密集向量，而输出层则尝试为上下文中的每个词分配概率

   - `GloVe`：基于全局词共现统计优化向量。

   - `FastText`：考虑子词信息（解决未登录词问题）。

3. **可自定义维度**：

   - 开发者可指定向量大小（如 50/100/300 维），维度越低训练越快但信息容量越小，需权衡任务需求

**Word2Vec加速方法**
一般神经网络语言模型在预测的时候，输出的是预测目标词的概率，通过softmax得到，也就是每一次预测都要基于全部的数据集进行计算，这无疑会带来很大的时间开销。例子如下:

- “苹果” -> “汁” 的logit为2.0
- “苹果” -> “手机” 的logit为1.5
- “苹果” -> “电脑” 的logit为0.5

经过softmax后->softmax概率分布: [0.65900117 0.24243298 0.09856585], 这意味着，在给定“苹果”这个词后，模型预测下一个词是“汁”的概率为65.9%，是“手机”的概率为24.2%，是“电脑”的概率为9.9%。这些概率之和为1

Word2Vec提出两种加快训练速度的方式，一种是Hierarchical softmax(霍夫曼树,用的不多)，另一种是Negative Sampling(负采样,常见)

**霍夫曼树:**

为了避免计算所有词的softmax概率，word2vec采用了霍夫曼树来代替从隐层到softmax层的映射，根据词频来建立哈夫曼树。将多分类转为二分类问题, 对所有词进行二进制编码, 特点如下:

叶子节点的个数就是词汇表的大小

不同词编码不同, 每个词的编码不会成为另一个词的前缀

词频越高编码长度越短

如:011011

优点:

1.计算效率高

2.高频词接近词根更容易被检索

缺陷:

如果我们的训练样本里的中心词是一个很生僻的词，那么就得在霍夫曼树中辛苦的向下走很久了。负采样可以解决该问题



**负采样(概使用率采样)**

对于给定的词W的上下文Context(w)，词w是一个正样本，其他词是负样本使用了sigmoid函数

当使用负采样时，我们将随机选择一小部分的negative words(比如选5个negative words)来更新对应的权重。我们也会对我们的positive word进行权重更新(上面的例子指的是"quick")

概率采样是一种根据词频进行随机抽样的方法，我们倾向于选择词频比较大的负样本。Word2vec采用0.75次幂来减小词频差异，使得词频较小的样本也有机会被采到。



##### 3. FastText

FastText 是一个基于词向量的快速文本分类算法，其结构与 CBOW 模型相似，都包含输入层、隐含层和输出层。两者的主要区别在于输入和输出的不同：

1. 输入：CBOW 模型的输入是目标单词的上下文，而 FastText 的输入则是多个单词及其 n-gram 特征，这些特征用来表示单个文档。CBOW 的输入单词是 one-hot 编码，而 FastText 的输入特征是被 embedding 过的。

2. 输出：CBOW 模型的输出是目标词汇，而 FastText 的输出则是文档对应的类标。

   

### 1.3 Attention注意力

1.3.4 KV Cache键值缓存

KV Cache应用于推理阶段(也就是K、V的值是不变的)

KV Cache只存在于Decoder解码器中
它的目的是为了加速Q@K@V的两次矩阵相乘时的速度
KV Cache会加大内存占用



**将之前序列token计算过的KV缓存下来用，这就是KV Cache技术**

![1753005556053](D:\TTT\NLP算法\笔记\298图片\1753005556053.png)

假设有1000个字, 最大维度512

![1753005877410](D:\TTT\NLP算法\笔记\298图片\1753005877410.png)





![1753006870104](C:\Users\TTT\AppData\Roaming\Typora\typora-user-images\1753006870104.png)

![1753007244720](D:\TTT\NLP算法\笔记\298图片\1753007244720.png)

KV Cache 优化方法
1.共用 KV cache: MQA，GQA 见1.3.5章 1.3 Attention 注意力2.窗口优化:
KV cache 的作用是计算 attention，当推理时的文本长度T大于训练时的最大长度L 时，一个自然的想法就是滑动窗口。这里又有三种方式:
。
固定窗口长度(图 b)，典型代表即 Longformer，该方法实现简单，而空间复杂度只有 O(TL)但是精度下降比较大
KV 重计算(图 c)，该方法需要每次计算都重新计算长度为L的 KV cache，由于重计算的存在。其精度可以保证，但是性能损失比较大
箭型 attention 窗口，在 LM-Infinit 中就已经被提出了，其基本原理和 StreamingLLM 是一致的。3.量化与稀疏:当前主流推理框架都在逐步支持 KV Cache 量化，一个典型的案例是 lmdeploy

4.存储与计算优化:
。 vLLM 的 PagedAttention，简单来说就是允许在非连续的内存空间中存储连续的 K和 VFlashDecoding 是在 FlashAttention 的基础上针对 inference 的优化主要分为三步:
长文本下将KV分成更小且方便并行的chunk国
”对每个chunk的KV，Q和他们进行之前一样的FlashAttention获取这个chunk的结果
对每个chunk的结果进行reduce


10.4.9 为什么BERT在第一句前会加一个[CLS]标志? BERT输出的两个函数是什么？​
•
BERT在第一句前会加一个[CLS]标志，最后一层该位置对应的向量可以作为整句话的语义表示，从而用于下游的分类任务等。与文本中已有的其它词相比，这个无明显语义信息的符号会更“公平”地融合文本中各个词的语义信息，从而更好的表示整句话的语义。具体来说，self-attention是用文本中的其它词来增强目标词的语义表示，但是目标词本身的语义还是会占主要部分的，因此，经过BERT的12层，每次词的embedding融合了所有词的信息，可以去更好的表示自己的语义。而[CLS]位本身没有语义，经过12层，得到的是attention后所有词的加权平均，相比其他正常词，可以更好的表征句子语义。当然，也可以通过对最后一层所有词的embedding做pooling表征句子语义​
•
在BERT TF源码中有两种输出​
◦
get_pooled_out()，就是上述[CLS]的表示，输出shape是[batch size,hidden size]​
◦
get_sequence_out()，获取的是整个句子每一个token的向量表示，输出shape是[batch_size, seq_length, hidden_size]，这里也包括[CLS]对应的向量

模型。​
10.4.44 大模型的困惑度（PPL）是什么？​
困惑度（Perplexity, PPL）用于衡量语言模型的好坏。模型 Model 在预训练测试集数据 
​
T={w1,w2,…,w N}
上的困惑度越低，说明模型对下一个单词的预测越准确，模型性能越好，生成文本的流畅性和一致性越高：​
                     
Perplexity(Model)=exp(− 
N
1
​
  
i=1
∑
N
​
 logP(w 
i
​
 ∣w 
1
​
 ,…,w 
i−1
​
 ))
​
其中
​
N
是测试数据的 token 总数， 
​
P(w 
i
​
 ∣w 
1
​
 ,⋯,w 
i−1
​
 )
是模型对第
​
i
个 token 的预测概率  ​
可以看到上述公式括号中的式子实际上就是预训练使用的交叉熵 Loss，所以 PPL 的计算也可以写成下式：​
​
PPL(Model)=exp(L)
​
利用transformers库简单计算逻辑如下，更详细的代码见：https://huggingface.co/docs/transformers/perplexity​
​
​ ​
代码块​
￼
Python
from transformers import AutoModelForCausalLM, AutoTokenizer​
import torch​
model = AutoModelForCausalLM.from_pretrained("gpt2")​
tokenizer = AutoTokenizer.from_pretrained("gpt2")​
text = "I am RZ, a LLMer and a RLer."​
inputs = tokenizer(text, return_tensors="pt")​
with torch.no_grad():​
    outputs = model(**inputs, labels=inputs["input_ids"])​
    loss = outputs.loss​
    ppl = torch.exp(loss).item()​
print(f"Perplexity: {ppl}")

对于长文本来说则需要进行分块处理：​
​
​ ​
代码块​
￼
Python
max_length = model.config.n_positions​
stride = 512  # 滑动窗口步长​
total_loss = 0​
total_count = 0​
for i in range(0, len(input_ids), stride):​
    chunk = input_ids[:, i:i+max_length]​
    with torch.no_grad():​
        outputs = model(chunk, labels=chunk)​
    loss = outputs.loss​
    total_loss += loss.item() * chunk.size(1)​
    total_count += chunk.size(1)​
ppl = torch.exp(total_loss / total_count).item()​
​ ​
​
在训练过程中监控验证集的PPL变化，可动态调整学习率或提前终止训练，也可以监控预训练多任务、多领域数据的训练情况（比如从每个数据集随机抽样出200个左右的样本，作为PPL监控的样本）。但是PPL指标依赖测试数据的分布，若数据与训练集差异大，PPL可能失真。而且全局平均值可能掩盖局部生成错误（如逻辑矛盾）。​
PPL与BLEU、ROUGE的区别？​
PPL基于概率评估，BLEU/ROUGE基于n-gram重叠率。PPL 反映模型内部置信度，其他两个衡量的是生成文本与参考文本的相似度



1.5 Positional Encoding 位置编码​
1.5.1 位置编码介绍​
​
​ ​
✏️
位置编码的需求​
RNN的结构包含了序列的时序信息，而Transformer却完全把时序信息给丢掉了，比如“他欠我100万”，和“我欠他100万”，两者的意思千差万别，故为了解决时序的问题，Transformer的作者用了一个绝妙的办法：位置编码(Positional Encoding)​
Attention计算是无向的，而各种位置信息在tokens间的关系是很重要的。对于人来说很容易知道tokens的位置信息，比如：​
•
绝对位置信息，a1是第一个token，a2是第二个token......​
•
相对位置信息，a2在a1的后面一位，a4在a2的后面两位......​
•
不同位置间的距离，a1和a3差两个位置，a1和a4差三个位置....​
因此，我们需要这样一种位置表示方式，满足于：​
•
它能用来表示一个token在序列中的绝对位置​
•
在序列长度不同的情况下，不同序列中token的相对位置/距离也要保持一致​
•
可以用来表示模型在训练过程中从来没有看到过的句子长度（长度外推问题）​
需要一个有界又连续的函数，最简单的，正弦函数sin就可以满足这一点。频率设成非常低，这样几乎不会有不同T的位置编码重合的情况了：​
​
PE 
t
​
 =[sin(w 
0
​
 t),sin(w 
1
​
 t),…,sin(w 
i−1
​
 t),…,sin(w 
d 
model
​
 −1
​
 t)]
​
​
w 
i
​
 = 
10000 
i/(d 
model
​
 −1)
 
1
​
 
​
​ ​
​
1.5.2 绝对位置编码​
Transformer的位置编码​
​
​ ​
✏️
Transformer的位置编码加在embedding上，但是由于使用的是sin cos 交替，可以通过线性变换矩阵得到其他位置的表示，所以可以期望他包含了相对位置的信息，而且由于三角函数有显示的生成规律，所以可以期望有外推性质​
•
计算公式：​
•
可视化：下图是长度为100，编码维度为512的序列的位置编码可视化，可以发现，由于sin/cos函数的性质，位置向量的每一个值都位于[-1, 1]之间。同时，纵向来看，图的右半边几乎都是黄色的，这是因为越往后的位置，频率越小，波长越长，所以不同的t对最终的结果影响不大。而越往左边走，颜色交替的频率越频繁​
​ ​
​
​
​ ​
￼
​ ​
​
​
​ ​
✏️
•
Transformer位置编码的缺点：由于位置编码点积的无向性， 
​
PE 
t
T
​
 ∗PE 
t+Δt
​
 =PE 
t
T
​
 ∗PE 
t−Δt
​
 
 即两个位置编码的乘积仅取决于 
​
ΔT
，距离是成对分布的，不能用来表示位置的方向性。当随着input embedding被喂入attention的时候会出现距离意识被破坏的现象，即正弦位置编码的相对位置表达能力被投影矩阵破坏掉了，所以在后续BERT的改进中，采用了可学习的位置编码​
​ ​
​
​
​ ​
￼
​ ​
​
​
BERT的可学习位置编码​
​
​ ​
✏️
直接将位置编码当作可训练参数，比如最大长度为512，编码维度为768，那么就初始化一个512×768的矩阵作为位置向量，让它随着训练过程更新。对于这种训练式的绝对位置编码，一般的认为它的缺点是没有长度外推性，即如果预训练最大长度为512的话，那么最多就只能处理长度为512的句子，再长就处理不了了。当然，也可以将超过512的位置向量随机初始化，然后继续微调​
具体示意见4.1章​​
🌲
4.1 BERT及变体​
​ ​
​
RNN 位置编码​
​
​ ​
✏️
递归式的绝对位置编码，input后面接一层RNN，可以用RNN学每个位置的位置编码。优点是外推、灵活，缺点是丧失transformed的并行处理性质


1.6.1 大模型结构​
​
​ ​
🗃️
首先回顾 Transformer 的结构如下所示，主要分为编码器 Encoder 和解码器 Decoder 两部分，且都是多层叠加​
•
Encoder：MHA + FFN + LN&Add，采用双向注意力机制，前后的token都能看到​
•
Decoder：先是 Masked MHA + LN&Add，单向注意力，当前token只能看到自己之前的token，防止未来信息泄露，预测下一个token只能利用之前的信息，然后接上 Cross MHA（Encoder-Decoder Attention Layer），通过编码器输出的上下文信息来关注解码器这里的序列的相关部分，最后解码器生成与输入匹配的输出序列


10.1.1 基础问题​
1.
Python数据类型：请解释Python中的常见数据类型，如列表、元组、集合和字典。它们之间有什么区别？​
◦
列表（List）：有序且可变的数据集合，使用方括号[]表示。​
◦
元组（Tuple）：有序且不可变的数据集合，使用圆括号()表示。此外不可变对象还有string、int、float、bool​
◦
集合（Set）：无序且不重复的数据集合，使用花括号{}表示。​
◦
字典（Dictionary）：键值对的无序集合，使用花括号{}表示，键是唯一的。​
2.
列表与元组的区别：在什么情况下你会选择使用列表而不是元组，反之亦然？​
◦
列表：可变，支持增删改操作，适合需要频繁修改的数据。​
◦
元组：不可变，创建后无法修改，适合存储不可变的数据，且因为不可变，所以速度比列表更快。​
3.
字符串操作：如何反转一个字符串？请写出相应的Python代码。​
​
​ ​
代码块​
￼
​ ​
​
4.
==和is有什么区别？​
◦
使用==操作符比较两个对象的值是否相等。​
◦
使用is操作符比较两个对象是否是同一个对象（即是否有相同的内存地址）​
5.
python是一种动态语言​
动态类型：在Python中，变量的类型是在运行时决定的，而不是在编译时决定的。这意味着你可以在程序运行期间改变变量的类型。​
动态类型检查：类型检查是在运行时进行的，而不是在编译时进行的。这使得Python代码更加灵活，但也意味着一些类型错误可能只会在运行时被发现。​
动态内存管理：Python使用垃圾回收机制来自动管理内存。这种动态的内存管理方式使得程序员不需要手动管理内存的分配和释放。​
与静态语言的对比​
静态语言（如C、C++、Java等）在编译时就需要确定变量的类型，并且在编译过程中进行类型检查。静态语言的这种特性可以在编译时捕捉到更多的错误，提高程序的安全性和性能。​
动态语言的优势和劣势​
优势​
a.
灵活性：动态语言允许更灵活的编程风格，适合快速开发和原型设计。​
b.
代码简洁：由于没有类型声明，代码更加简洁，减少了代码量。​
c.
易于调试：可以在运行时动态地检查和修改对象，有助于调试和开发。​
劣势​
a.
运行时错误：类型错误和其他问题只有在运行时才会被发现，可能导致程序在运行时崩溃。​
b.
性能：由于动态类型检查和其他动态特性，动态语言的运行速度通常比静态语言慢。​
Python是一种动态语言，因为它在运行时才确定变量的类型，并进行类型检查。这种特性使得Python灵活且易于使用，适合快速开发和原型设计，但也带来了运行时错误和性能方面的挑战。​
6.
拷贝和深拷贝？​
◦
直接赋值：其实就是对象的引用（别名）。​
◦
Python中对象的赋值都是进行对象引用（内存地址）传递。​
◦
使用copy.copy()，可以进行对象的浅拷贝，它复制了对象，但对于对象中的元素，依然使用原始的引用。​
◦
如果需要复制一个容器对象，以及它里面的所有元素（包含元素的子元素），可以使用copy.deepcopy()进行深拷贝。​
◦
对于非容器类型（如数字、字符串、和其他'原子'类型的对象）没有被拷贝一说。​
◦
如果元组变量只包含原子类型对象，则不能深拷贝。​
​
​ ​
代码块​
￼import copy​
a = [1, 2, 3, 4, ['a', 'b']]​
b = a #赋值，传对象的引用​
c = copy.copy(a) #浅拷贝，对象拷贝​
d = copy.deepcopy(a) #深拷贝，对象拷贝​
 ​
a.append(5) #修改对象a,列表末尾添加数字5​
a[4].append('c') #修改对象a中的列表['a', 'b']​
 ​
print 'a= ', a​
print 'b= ', b​
print 'c= ', c​
print 'd= ', d​
 ​
输出结果：​
a = [1, 2, 3, 4, ['a', 'b', 'c'], 5] #修改了原始对象，变为修改后的列表​
b = [1, 2, 3, 4, ['a', 'b', 'c'], 5] #对象的引用，对象的任何值变化都随着变化​
c = [1, 2, 3, 4, ['a', 'b', 'c']] #浅拷贝，不可变元素不能改变，可变元素随着原始对的变化而变化了​
d = [1, 2, 3, 4, ['a', 'b']] #深
​ ​
​
7.
*Args 与 **kwargs？​
◦
*args是可变的positional arguments列表，**kwargs是可变的keyword arguments列表;​
◦
*args必须位于**kwargs之前，因为positional arguments必须位于keyword arguments之前;​
◦
*args表示任何多个无名参数，它是一个tuple；**kwargs表示关键字参数，它是一个dict。​
8.
简述python是一种解释语言吗？​
a.
逐行执行：解释型语言在运行时逐行解释代码，这意味着源代码不需要先编译成二进制的可执行文件。Python代码在执行时被解释器（如CPython、PyPy等）逐行翻译成机器码并立即执行。​
b.
跨平台性：由于Python代码由解释器执行，只要有相应的Python解释器，就可以在不同的操作系统上运行同样的Python代码。这使得Python具有很好的跨平台性。​
c.
开发效率高：由于不需要编译步骤，开发者可以快速地编写和测试代码，从而提高开发效率。​
d.
动态类型：Python是动态类型语言，变量的类型在运行时决定，这与解释型语言的动态特性相吻合。​
e.
即时执行：Python代码在运行时由解释器逐行解释执行，无需提前编译。​
9.
python的参数转递：​
 位置参数、关键字参数、默认参数、可变长度参数​
10.
静态方法和类方法：​
◦
@staticmethod：​
▪
不需要访问类实例或类本身。​
▪
可以通过类或实例调用。​
▪
用于实现与类相关但不需要访问类或实例的功能。​
◦
@classmethod：​
▪
第一个参数是类对象（cls）。​
▪
可以通过类或实例调用。​
▪
用于访问和修改类状态，或者实现一些与类相关的操作​
11.
python类变量和实例变量​
12.
字典推导式：​
​
​ ​
代码块​
￼{key_expr: value_expr for item in iterable if condition}​
keys = ['a', 'b', 'c']​
values = [1, 2, 3]​
dictionary = {k: v for k, v in zip(keys, values)}​
print(dictionary)  # 输出：{'a': 1, 'b': 2, 'c': 3}
​ ​
​
13.
什么是python函数式编程？​
函数式编程（Functional Programming）是一种编程范式，它把计算视为数学函数的求值，避免了使用状态和可变数据。函数式编程强调函数的使用，并且函数被视为第一类对象（first-class citizens），这意味着函数可以赋值给变量、作为参数传递以及作为返回值。​
Python虽然不是纯函数式编程语言，但它支持许多函数式编程的特性和概念。map对可迭代对象中的每个元素应用一个函数，并返回一个迭代器。​
◦
函数可以像变量一样传递和操作。​
◦
函数可以接受其他函数作为参数，或者返回一个函数。​
◦
使用lambda关键字定义匿名函数。​
14.
模块和包：模块是包含Python代码的文件，包是一个包含多个模块的目录​
15.
pickling:转换为字节流，pickle.dump，unpickling:逆运算，pickle.load​
16.
range返回一个惰性序列，不会一次性把所有数都生成了​
17.
reduce函数：reduce函数会对序列中的元素进行累积操作，从而将序列简化为单一的值。它通过反复调用指定的二元函数，将序列中的前两个元素传递给该函数，然后将该函数的结果与下一个元素一起传递给函数，如此往复，直到序列中的所有元素都被处理完。​
18.
filter函数：返回的是一个迭代器​
19.
Python是按引用调用还是按照值调用？:​
a.
可变对象和不可变对象：​
▪
可变对象（如列表、字典、集合等）：在函数内部对可变对象的修改会影响到原始对象。​
▪
不可变对象（如整数、字符串、元组等）：在函数内部对不可变对象的任何操作不会影响到原始对象，因为这些操作实际上会创建一个新的对象。​
b.
参数传递机制：​
▪
当一个对象作为参数传递给函数时，实际上是传递了这个对象的引用（即对象的内存地址）。​
▪
如果传递的是一个可变对象，在函数内部对这个对象的修改会影响到原始对象，因为函数内部和外部的变量都指向同一个对象。​
▪
如果传递的是一个不可变对象，在函数内部对这个对象的修改不会影响到原始对象，因为任何修改都会创建一个新的对象，而不会改变原有对象。​
20.
any()检查是否存在一个True，all()检查是否都是True​
21.
Python如何将两个列表组合成一个元组列表？list(zip(list1, list2))​
22.
删除相关：​
◦
remove：​
▪
用于删除第一个匹配的元素。​
▪
按值删除，不返回被删除的元素。​
▪
如果元素不存在，会引发 ValueError。​
◦
del：​
▪
用于删除指定索引的元素、切片范围，或整个列表。​
▪
不返回被删除的元素。​
▪
如果索引超出范围，会引发 IndexError。​
◦
pop：​
▪
用于删除并返回指定索引的元素，默认是最后一个元素。​
▪
返回被删除的元素。​
▪
如果索引超出范围，会引发 IndexError。​
23.
可迭代对象和迭代器：​
可迭代对象（Iterable）​
◦
定义：可迭代对象是一个可以返回迭代器的对象。任何实现了 iter() 或者 getitem() 方法的对象都是可迭代的。​
◦
例子：常见的可迭代对象包括列表（list）、元组（tuple）、集合（set）、字典（dictionary）、字符串（string）等。​
迭代器（Iterator）​
◦
定义：迭代器是一个实现了 iter() 和 next() 方法的对象。迭代器对象能够记住遍历的位置，next() 方法返回下一个元素，如果没有元素则引发 StopIteration 异常。​
◦
创建方法：通过调用可迭代对象的 iter() 方法，可以得到一个迭代器。​
​
​ ​
代码块​
￼
​ ​my_list = [1, 2, 3, 4]​
# 获取迭代器对象​
my_iterator = iter(my_list)​
# 使用迭代器逐个访问元素​
print(next(my_iterator))  # 输出：1​
print(next(my_iterator))  # 输出：2​
print(next(my_iterator))  # 输出：3​
print(next(my_iterator))  # 输出：4
​
24.
字符串、列表、元组、字典每个最常用的五个方法？​
   字符串：split、join、strip、replace、find​
   列表：append、extend、pop、remove、sort​
   元组：count、index​
   字典：get、keys、values、items、update​
25.
简述Python多线程共同操作同一个数据互斥锁同步？​
 在Python中，多线程操作同一个数据时，为了避免竞争条件和数据不一致问题，通常需要使用互斥锁（Mutex Lock）来同步线程的访问。互斥锁确保在任何时刻只有一个线程能够访问共享资源，从而保证数据的完整性和一致性。互斥锁是一种同步原语，用于保护共享资源。互斥锁确保同一时间只有一个线程可以访问共享资源。Python的threading模块提供了Lock类来实现互斥锁。​
a.
创建锁对象：使用threading.Lock()创建一个锁对象。​
b.
获取锁：在线程访问共享资源之前，使用lock.acquire()获取锁。​
c.
释放锁：在线程访问共享资源之后，使用lock.release()释放锁。​
◦
避免死锁：确保每次成功获取锁后都能释放锁，以避免死锁情况。使用try-finally块确保锁的释放。​
26.
Python线程同步：在多线程编程中，线程同步是确保多个线程正确、顺序地访问共享资源的重要机制。Python提供了多种线程同步原语来解决多线程竞争和数据不一致的问题​
◦
互斥锁（Lock）：用于保护共享资源，确保同一时刻只有一个线程可以访问。​
◦
可重入锁（RLock）：允许同一线程多次获取锁，适用于递归调用。​
◦
信号量（Semaphore）：控制对资源的访问量，适用于限制并发线程数。​
◦
条件变量（Condition）：用于线程间的通信和同步，适用于复杂的等待和通知机制。​
◦
事件（Event）：用于线程间的简单同步和等待事件的发生。​
27.
Python死锁：死锁（Deadlock）是多线程或多进程编程中的一种现象，指的是两个或多个线程或进程因相互等待对方释放资源而陷入无限期等待的状态。换句话说，死锁发生时，这些线程或进程都无法继续执行，因为每一个都在等待其他线程或进程持有的资源。​
a.
避免环路等待条件：通过一次性获取所有需要的锁，避免环路等待。例如，可以使用一个排序的策略，确保所有线程按照相同的顺序获取锁。​
b.
使用超时：在尝试获取锁时使用超时参数，以避免无限期等待。​
◦
线程安全：在多线程环境中，程序能正确执行，不会因并发操作导致不一致或错误。​
◦
互斥锁（Mutex Lock）：一种同步原语，用于保护共享资源，确保同一时刻只有一个线程访问资源。通过lock.acquire()和lock.release()操作控制线程对资源的访问。​
◦
实现线程安全的替代方案：使用线程安全的数据结构、高层次的并发工具，或避免共享状态等。​
◦
进程：​
▪
并发：在单核处理器上，通过任务切换实现。​
▪
并行：在多核处理器上，通过在不同处理器核心上同时运行多个进程实现。​
◦
线程：​
▪
并发：在单核处理器上，通过任务切换实现。​
▪
并行：在多核处理器上，通过在不同处理器核心上同时运行多个线程实现，但受限于Python的GIL，真正的并行仅限于I/O密集型任务。​
28.
Python asyncio:asyncio 是 Python 中用于编写并发代码的标准库，它使用异步 I/O 使程序能够处理大量的 I/O 操作而无需阻塞。asyncio 提供了一个事件循环，用于调度和执行异步任务，实现高效的并发。​
◦
事件循环：asyncio 的核心，负责调度和执行异步任务。​
◦
协程：使用 async def 定义，异步执行的函数，可以在执行过程中挂起和恢复。​
◦
任务：协程的封装，表示协程的执行，可以由事件循环调度。​
◦
异步 I/O：通过 await 关键字等待异步操作，不会阻塞事件循环。​
通过使用 asyncio，可以编写高效的异步代码，在处理大量 I/O 操作时显著提高性能和响应速度。​
29.
Metaclass​
◦
元类（Metaclass）：用于创建类的“类”，定义了类的创建和行为方式。​
◦
基本概念：类本身是由元类创建的对象，默认的元类是type。​
◦
定制类行为：通过重载元类的new和init方法，可以控制类的创建过程和行为。​
◦
应用场景：单例模式、自动注册类等。​
10.1.2 高级主题​
1.
生成器和迭代器：请解释生成器和迭代器的区别，并给出一个生成器函数的例子。​
https://blog.csdn.net/Hardworking666/article/details/112061653​
这个讲得好：https://blog.csdn.net/weixin_44706915/article/details/116702292​
◦
迭代器（Iterator）：是一种对象，实现了iter()和next()方法，可以在for循环中使用。​
◦
生成器（Generator）：是一种特殊的迭代器，使用yield语句返回值，可以暂停和恢复函数的执行。​
​
​ ​
代码块​
￼
​ ​
​
2.
装饰器：什么是装饰器？请实现一个简单的装饰器，并说明它的作用。​
◦
装饰器（Decorator）：是一种用于修改函数或方法行为的函数，通常用于日志记录、访问控制、性能监测等。​
​
​ ​
代码块​
￼
​ ​
​
3.
并发编程：Python中有哪些实现并发编程的方法？请简单介绍一下线程、进程和协程。​
◦
线程（Thread）：轻量级进程，使用threading模块实现。​
◦
进程（Process）：独立的进程，使用multiprocessing模块实现。​
◦
协程（Coroutine）：一种轻量级线程，使用asyncio模块实现，适用于I/O密集型任务。异步编程​
     线程、进程和协程的概念和区别：https://blog.csdn.net/daaikuaichuan/article/details/82951084​
     python GIL概念和原理：https://blog.csdn.net/qq_41586251/article/details/135117057​
     python并发编程：https://blog.csdn.net/weixin_52906070/article/details/132317118​
进程间通信原理：https://blog.csdn.net/xukris/article/details/135688894​
​
​ ​
代码块​
￼
​ ​
​
10.1.3 语法和基础​
1.
Python的变量作用域：请解释Python中局部变量和全局变量的区别，如何在函数内部修改全局变量？​
 修改全局变量：使用global关键字。​
​
​ ​
代码块​
￼
​ ​
​
2.
Python中的内置函数：列举并简要解释Python中的五个常用内置函数。​
◦
len()：返回对象的长度。​
◦
type()：返回对象的类型。​
◦
print()：打印输出。​
◦
range()：生成一个范围对象。​
◦
sum()：计算序列的和。​
3.
列表推导式：什么是列表推导式？请给出一个包含条件和嵌套的列表推导式的例子。​
简洁的创建列表的方法​
​
​ ​
代码块​
￼
​ ​
​
10.1.4 面向对象编程​
1.
类与对象：请解释Python中类和对象的概念，并实现一个简单的类。​
2.
继承和多态：请解释继承和多态的概念，并举例说明如何在Python中实现多态。​
3.
魔术方法：什么是魔术方法？请解释init、str和repr的作用，并举例说明如何使用它们。​
◦
魔术方法（Magic Methods）：特殊的方法，带有双下划线，用于重载操作符或实现特定行为。​
◦
init：初始化方法。​
◦
str：返回对象的可读字符串表示。​
◦
repr：返回对象的官方字符串表示。​
10.1.5 内存管理和优化​
1.
垃圾回收机制：请解释Python的垃圾回收机制是如何工作的。​
◦
Python使用引用计数和循环垃圾收集机制来管理内存。​
◦
当对象的引用计数为0时，该对象会被垃圾回收。​
◦
循环垃圾收集器可以检测和回收循环引用的对象。​
2.
内存泄漏：什么是内存泄漏？如何在Python中避免内存泄漏？​
◦
内存泄漏是指程序中已经不再使用的内存没有被释放。​
◦
在Python中，内存泄漏通常由循环引用或对全局对象的引用造成。​
◦
可以使用gc模块强制进行垃圾回收。​
3.
性能优化：在Python中进行性能优化时，有哪些常见的方法和工具？​
◦
使用内置函数和库：例如，使用map()代替手动循环。​
◦
列表推导式：相比于传统的循环，列表推导式更高效。​
◦
减少全局变量的使用：全局变量会增加访问时间。​
◦
使用生成器：生成器比列表更节省内存。​
10.1.6 错误和异常处理​
1.
异常处理：请解释Python中的异常处理机制，并写出一个包含try-except-finally结构的示例。​
Python使用try-except-finally结构处理异常。​
​
​ ​
代码块​
￼
​ ​
​
2.
自定义异常：如何在Python中创建和使用自定义异常？​
​
​ ​
代码块​
￼
​ ​
​
3.
断言：请解释assert语句的作用，并举例说明其使用场景。​
assert语句用于检查条件是否为真，若不为真则抛出AssertionError。​
10.1.7 模块和包​
1.
模块导入：请解释如何在Python中导入模块，什么是相对导入和绝对导入？​
2.
包的结构：请描述一个Python包的基本结构，并解释init.py文件的作用。​
3.
虚拟环境：什么是虚拟环境？如何创建和管理Python虚拟环境？​
10.1.8 函数和高级特性​
1.
匿名函数：什么是匿名函数（Lambda函数）？请给出一个使用匿名函数的例子。​
简洁性：Lambda函数允许在一行中定义简单的函数，代码更加简洁和紧凑。​
代码可读性：在一些简单的情况下，lambda函数可以使代码更易读。例如，在高阶函数（如map、filter、sorted）中使用lambda函数，能使意图更加清晰。​
​
​ ​
代码块​
￼
​ ​
​
2.
装饰器：请解释装饰器的概念，并实现一个记录函数执行时间的装饰器。​
​
​ ​
代码块​
￼
​ ​
​
3.
上下文管理器：什么是上下文管理器？请实现一个自定义的上下文管理器。​
 上下文管理器用于管理资源的分配和释放，常用于文件操作。​
​
​ ​
代码块​
￼
class MyContextManager:​
    def __enter__(self):​
        print("Entering context")​
        return self​
    def __exit__(self, exc_type, exc_value, traceback):​
        print("Exiting context")​
with MyContextManager():​
    print("Inside context")​
​ ​
​
10.1.9 库和框架​
1.
常用标准库：列举并简要介绍Python的几个常用标准库，例如os、sys、re、json。​
◦
os：与操作系统交互的接口。​
◦
sys：访问与Python解释器相关的变量和函数。​
◦
re：正则表达式操作。​
◦
json：JSON编码和解码。​
◦
datetime：处理日期和时间。​
2.
请求库：请解释如何使用requests库进行HTTP请求，并举例说明如何发送GET和POST请求。​
使用requests库进行HTTP请求。​
​
​ ​
代码块​
￼
import requests​
# 发送GET请求​
response = requests.get('https://api.github.com')​
print(response.status_code)​
# 发送POST请求​
response = requests.post('https://httpbin.org/post', data={'key': 'value'})​
print(response.json())​
​ ​
​
3.
数据处理：请简要介绍pandas库的基本功能，并实现一个简单的数据处理示例。​
​
​ ​
代码块​
￼
import pandas as pd​
# 创建数据框​
data = {'Name': ['Alice', 'Bob', 'Charlie'], 'Age': [25, 30, 35]}​
df = pd.DataFrame(data)​
# 选择列​
print(df['Name'])​
# 过滤行​
print(df[df['Age'] > 30])​
​ ​
​
 RNN、CNN和Transformer三大NLP特征提取器对比

RNN:采用线性序列结构不断从前往后收集输入信息，但这种线性序列结构在反向传播的时候存在优化困难问题，因为反向传播路径太长，容易导致严重的梯度消失或梯度爆炸问题.。RNN结构（LSTM，GRU）结构存在序列依赖，对大规模并行非常不友好

CNN：卷积层其实是保留了相对位置信息的，并行能力比较强

RNN，CNN，Transformer对比：

语义特征提取能力：Transformer在这方面的能力非常显著地超过RNN和CNN，RNN和CNN两者能力差不太多

长距离特征捕获能力:：原生CNN特征抽取器在这方面极为显著地弱于RNN和Transformer，Transformer微弱优于RNN模型(尤其在主语谓语距离小于13时)，能力由强到弱排序为Transformer>RNN>>CNN; 但在比较远的距离上（主语谓语距离大于13），RNN微弱优于Transformer，所以综合看，可以认为Transformer和RNN在这方面能力差不太多，而CNN则显著弱于前两者

任务综合特征抽取能力: （机器翻译）：Transformer综合能力要明显强于RNN和CNN，而RNN和CNN看上去表现基本相当，貌似CNN表现略好一些

并行计算能力及运行效率:：RNN在并行计算方面有严重缺陷，这是它本身的序列依赖特性导致的；对于CNN和Transformer来说，因为它们不存在网络中间状态不同时间步输入的依赖关系，所以可以非常方便及自由地做并行计算改造。Transformer和CNN差不多，都远远远远强于RNN

**单从任务综合效果方面来说，Transformer明显优于CNN，CNN略微优于RNN。速度方面Transformer和CNN明显占优，RNN在这方面劣势非常明显**
​
