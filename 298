10.4.9 为什么BERT在第一句前会加一个[CLS]标志? BERT输出的两个函数是什么？​
•
BERT在第一句前会加一个[CLS]标志，最后一层该位置对应的向量可以作为整句话的语义表示，从而用于下游的分类任务等。与文本中已有的其它词相比，这个无明显语义信息的符号会更“公平”地融合文本中各个词的语义信息，从而更好的表示整句话的语义。具体来说，self-attention是用文本中的其它词来增强目标词的语义表示，但是目标词本身的语义还是会占主要部分的，因此，经过BERT的12层，每次词的embedding融合了所有词的信息，可以去更好的表示自己的语义。而[CLS]位本身没有语义，经过12层，得到的是attention后所有词的加权平均，相比其他正常词，可以更好的表征句子语义。当然，也可以通过对最后一层所有词的embedding做pooling表征句子语义​
•
在BERT TF源码中有两种输出​
◦
get_pooled_out()，就是上述[CLS]的表示，输出shape是[batch size,hidden size]​
◦
get_sequence_out()，获取的是整个句子每一个token的向量表示，输出shape是[batch_size, seq_length, hidden_size]，这里也包括[CLS]对应的向量

模型。​
10.4.44 大模型的困惑度（PPL）是什么？​
困惑度（Perplexity, PPL）用于衡量语言模型的好坏。模型 Model 在预训练测试集数据 
​
T={w1,w2,…,w N}
上的困惑度越低，说明模型对下一个单词的预测越准确，模型性能越好，生成文本的流畅性和一致性越高：​
                     
Perplexity(Model)=exp(− 
N
1
​
  
i=1
∑
N
​
 logP(w 
i
​
 ∣w 
1
​
 ,…,w 
i−1
​
 ))
​
其中
​
N
是测试数据的 token 总数， 
​
P(w 
i
​
 ∣w 
1
​
 ,⋯,w 
i−1
​
 )
是模型对第
​
i
个 token 的预测概率  ​
可以看到上述公式括号中的式子实际上就是预训练使用的交叉熵 Loss，所以 PPL 的计算也可以写成下式：​
​
PPL(Model)=exp(L)
​
利用transformers库简单计算逻辑如下，更详细的代码见：https://huggingface.co/docs/transformers/perplexity​
​
​ ​
代码块​
￼
Python
from transformers import AutoModelForCausalLM, AutoTokenizer​
import torch​
model = AutoModelForCausalLM.from_pretrained("gpt2")​
tokenizer = AutoTokenizer.from_pretrained("gpt2")​
text = "I am RZ, a LLMer and a RLer."​
inputs = tokenizer(text, return_tensors="pt")​
with torch.no_grad():​
    outputs = model(**inputs, labels=inputs["input_ids"])​
    loss = outputs.loss​
    ppl = torch.exp(loss).item()​
print(f"Perplexity: {ppl}")

对于长文本来说则需要进行分块处理：​
​
​ ​
代码块​
￼
Python
max_length = model.config.n_positions​
stride = 512  # 滑动窗口步长​
total_loss = 0​
total_count = 0​
for i in range(0, len(input_ids), stride):​
    chunk = input_ids[:, i:i+max_length]​
    with torch.no_grad():​
        outputs = model(chunk, labels=chunk)​
    loss = outputs.loss​
    total_loss += loss.item() * chunk.size(1)​
    total_count += chunk.size(1)​
ppl = torch.exp(total_loss / total_count).item()​
​ ​
​
在训练过程中监控验证集的PPL变化，可动态调整学习率或提前终止训练，也可以监控预训练多任务、多领域数据的训练情况（比如从每个数据集随机抽样出200个左右的样本，作为PPL监控的样本）。但是PPL指标依赖测试数据的分布，若数据与训练集差异大，PPL可能失真。而且全局平均值可能掩盖局部生成错误（如逻辑矛盾）。​
PPL与BLEU、ROUGE的区别？​
PPL基于概率评估，BLEU/ROUGE基于n-gram重叠率。PPL 反映模型内部置信度，其他两个衡量的是生成文本与参考文本的相似度



1.5 Positional Encoding 位置编码​
1.5.1 位置编码介绍​
​
​ ​
✏️
位置编码的需求​
RNN的结构包含了序列的时序信息，而Transformer却完全把时序信息给丢掉了，比如“他欠我100万”，和“我欠他100万”，两者的意思千差万别，故为了解决时序的问题，Transformer的作者用了一个绝妙的办法：位置编码(Positional Encoding)​
Attention计算是无向的，而各种位置信息在tokens间的关系是很重要的。对于人来说很容易知道tokens的位置信息，比如：​
•
绝对位置信息，a1是第一个token，a2是第二个token......​
•
相对位置信息，a2在a1的后面一位，a4在a2的后面两位......​
•
不同位置间的距离，a1和a3差两个位置，a1和a4差三个位置....​
因此，我们需要这样一种位置表示方式，满足于：​
•
它能用来表示一个token在序列中的绝对位置​
•
在序列长度不同的情况下，不同序列中token的相对位置/距离也要保持一致​
•
可以用来表示模型在训练过程中从来没有看到过的句子长度（长度外推问题）​
需要一个有界又连续的函数，最简单的，正弦函数sin就可以满足这一点。频率设成非常低，这样几乎不会有不同T的位置编码重合的情况了：​
​
PE 
t
​
 =[sin(w 
0
​
 t),sin(w 
1
​
 t),…,sin(w 
i−1
​
 t),…,sin(w 
d 
model
​
 −1
​
 t)]
​
​
w 
i
​
 = 
10000 
i/(d 
model
​
 −1)
 
1
​
 
​
​ ​
​
1.5.2 绝对位置编码​
Transformer的位置编码​
​
​ ​
✏️
Transformer的位置编码加在embedding上，但是由于使用的是sin cos 交替，可以通过线性变换矩阵得到其他位置的表示，所以可以期望他包含了相对位置的信息，而且由于三角函数有显示的生成规律，所以可以期望有外推性质​
•
计算公式：​
•
可视化：下图是长度为100，编码维度为512的序列的位置编码可视化，可以发现，由于sin/cos函数的性质，位置向量的每一个值都位于[-1, 1]之间。同时，纵向来看，图的右半边几乎都是黄色的，这是因为越往后的位置，频率越小，波长越长，所以不同的t对最终的结果影响不大。而越往左边走，颜色交替的频率越频繁​
​ ​
​
​
​ ​
￼
​ ​
​
​
​ ​
✏️
•
Transformer位置编码的缺点：由于位置编码点积的无向性， 
​
PE 
t
T
​
 ∗PE 
t+Δt
​
 =PE 
t
T
​
 ∗PE 
t−Δt
​
 
 即两个位置编码的乘积仅取决于 
​
ΔT
，距离是成对分布的，不能用来表示位置的方向性。当随着input embedding被喂入attention的时候会出现距离意识被破坏的现象，即正弦位置编码的相对位置表达能力被投影矩阵破坏掉了，所以在后续BERT的改进中，采用了可学习的位置编码​
​ ​
​
​
​ ​
￼
​ ​
​
​
BERT的可学习位置编码​
​
​ ​
✏️
直接将位置编码当作可训练参数，比如最大长度为512，编码维度为768，那么就初始化一个512×768的矩阵作为位置向量，让它随着训练过程更新。对于这种训练式的绝对位置编码，一般的认为它的缺点是没有长度外推性，即如果预训练最大长度为512的话，那么最多就只能处理长度为512的句子，再长就处理不了了。当然，也可以将超过512的位置向量随机初始化，然后继续微调​
具体示意见4.1章​​
🌲
4.1 BERT及变体​
​ ​
​
RNN 位置编码​
​
​ ​
✏️
递归式的绝对位置编码，input后面接一层RNN，可以用RNN学每个位置的位置编码。优点是外推、灵活，缺点是丧失transformed的并行处理性质


1.6.1 大模型结构​
​
​ ​
🗃️
首先回顾 Transformer 的结构如下所示，主要分为编码器 Encoder 和解码器 Decoder 两部分，且都是多层叠加​
•
Encoder：MHA + FFN + LN&Add，采用双向注意力机制，前后的token都能看到​
•
Decoder：先是 Masked MHA + LN&Add，单向注意力，当前token只能看到自己之前的token，防止未来信息泄露，预测下一个token只能利用之前的信息，然后接上 Cross MHA（Encoder-Decoder Attention Layer），通过编码器输出的上下文信息来关注解码器这里的序列的相关部分，最后解码器生成与输入匹配的输出序列
