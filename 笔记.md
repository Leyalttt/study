## 编程小知识点汇总

##### 1.randint(a, b)函数返回一个范围在[a, b]内的随机整数，包括a和b

```python
# - window_size确保从该起始位置开始的文本窗口能够完全位于给定的语料库（corpus）内，而不会超出其边界
start = random.randint(0, len(corpus) - 1 - window_size)
```



##### 2.为什么有些返回 y_pred有些需要torch.softmax(y_pred, dim=-1)

```python
# torch.softmax参数指定了要计算softmax的维度。在PyTorch中，dim=-1 表示最后一个维度
# y_pred：这是模型的原始输出，通常称为logits或分数。这些值没有经过归一化，因此它们的范围可以是任意的，并且它们的和不必为1。
# torch.softmax(y_pred, dim=-1)：这是对y_pred应用softmax函数的结果。softmax函数将logits转换为概率分布，
# 其中每个概率值都在0和1之间，并且所有概率值的和为1。这对于解释模型的输出非常有用，因为每个概率值现在可以直接解释为对应类别
# （在这个案例中是词汇表中的单词）出现的可能性
return torch.softmax(y_pred, dim=-1)
```

##### 3.创建一个从0到len(x)-1的整数数组

```python
x = 5
np.array(list(range(len(x))))  
array([0, 1, 2, 3, 4])
# 这里是将原生list转化为numpy的list
# numpy list优点：效率高
```

##### 4.x.reshape转换形状 + x.swapaxes行列转置

```python
x.shape = 4*768
x = x.reshape(4, 12, 64)
x = x.swapaxes(1, 0) # 第一维和第二位进行交换
L*768 -> L*12*64 -> 12*L*64只能这么转换
不能直接由
L*768-> 12* L* 64 这是不对的
x = x.reshape(-1, 768) # -1自动算出是4
```

##### 5.线性层 np.dot

```python
# np.dot两个矩阵点击 T为转置
q = np.dot(x, q_w.T) + q_b 
```

##### 6.np.matmul

通常和np.dot差不多

```python
qk = np.matmul(q, k.swapaxes(1, 2))
```

##### 7.将张量转成标量

```python
loss.item() #将张量转成标量
```

##### 8.random.sample 不放回

##### random.choice放回

```python
# 随机取sentence_length个，X不重复
x = random.sample(list(vocab.keys()), sentence_length)
# choice 返回元素本身 固定选 1 个, 多次调用可能会重复
lst = [1, 2, 3, 4]
result = random.choice(lst)  # 可能是 1、2、3 或 4
```

##### 9.模型预测的y_pred都在with torch.no_grad():  # 不计算梯度里

```python
with torch.no_grad():  # 不计算梯度    
  # model模型中的参数为张量
  result = model.forward(torch.LongTensor(x))  # 模型预测
  或者
  result = model(torch.LongTensor(x))  # 模型预测 
```

##### ==10.x[1:3]不包含下标3==

##### r不会使用转义字符

##### os.listdir返回目录下所有文件和文件夹

```python
dir = r"category_corpus/category_corpus"
files = os.listdir(dir)  # ['auto.txt', 'finance.txt', 'health.txt', 'science.txt', 'sports.txt', 'world.txt', 'a']
# a是文件夹
```

##### ==11.OS==

##### os.listdir(dir_path)列出目录 dir_path 下的所有文件和文件夹,返回列表

##### os.path.basename获取最后一级文件名和文件夹名

##### os.path.join拼接路径

##### os.path.isdir(config["model_path"]):路径是否存在

##### os.mkdir(config["model_path"]) 创建该目录

```python
import os
dir_path = r'category_corpus'
corpus = []
name = []
for path in os.listdir(dir_path):
    file_path = os.path.join(dir_path, path)
    # print('file_path', file_path)
    # #file_path category_corpus\a    # file_path category_corpus\finance.txt......

    # 文件是否以txt结尾
    if path.endswith("txt"):
        # 只读模式打开文件, 读取文件的全部内容并返回字符串。
        # print(open(file_path, 'r', encoding='utf-8').read())    # 世界，尽在于心：全新梅赛德斯-奔驰S级轿车今日上市 下一个特斯拉？蔚来汽车受国际资本追捧 投资者梦想发大财
    	corpus.append(open(file_path, 'r', encoding='utf-8').read())
    	name.append(os.path.basename(file_path))
print('corpus', corpus) # ['世界，尽在于心：全新梅赛德斯-奔驰S级轿车今日上市\n下一个特斯拉？....] 一个数组里包含多个文件的文本
print('name', name)  # ['auto.txt', 'finance.txt', 'health.txt', 'science.txt', 'sports.txt', 'world.txt']

# 检查 config 字典中 model_path 键对应的路径是否存在，并且该路径是否是一个目录。如果路径不存在或者不是一个目录，则返回 False
if not os.path.isdir(config["model_path"]):
    # os.mkdir 函数创建该目录
    os.mkdir(config["model_path"])
```

##### 12.遍历列表的index和value 要使用enumerate

```python
corpus = ['apple', 'banana', 'cherry']

for text_index, text_word in enumerate(corpus):
    print(f"Index: {text_index}, Word: {text_word}")
    # Index: 0, Word: apple
    # Index: 1, Word: banana
    # Index: 2, Word: cherry

```

##### 13.遍历dict中的key和value 需要将dict转为dict.items（）

```python
# 创建一个字典
my_dict = {'name': 'Alice', 'age': 25, 'city': 'New York'}

# 使用 items() 方法遍历字典的键值对
for key, value in my_dict.items():
    print(f"Key: {key}, Value: {value}")
    Key: name, Value: Alice
    Key: age, Value: 25
    Key: city, Value: New York

```

##### 14.字典get方法，通过key获取value

```python
d = {'a': 1, 'b': 2, 'c': 3}
value = d.get('b')
print(value)  # 输出：2去
```

##### 15.引入其他文件的方法

```python
from calculate_tfidf import calculate_tfidf
```

##### 16.rnn用法

```python
# vector_dim输入特征和vector_dim隐藏状态的维度相同
self.rnn = nn.RNN(vector_dim, vector_dim, batch_first=True)
rnn_out, hidden = self.rnn(x)
# 使用rnn的情况 每个字rnn_out RNN 在处理输入序列后的输出(预测结果/结果分类), 最终的隐藏状态hidden(包含了在每个时间步的隐藏状态)
# batch_first为false的形状 sentence_length(文本长度) * batch_size(每次训练样本个数)  * char_dim(每个字的维度)
# batch_first为true的形状  batch_size(每次训练样本个数) * sentence_length(文本长度) * char_dim(每个字的维度)
rnn_out, hidden = self.rnn(x)
# print('rnn_out', rnn_out.shape) # 40*10*30 batch_size(每次训练样本个数) * sentence_length(文本长度) * char_dim(每个字的维度)
# print('hidden', hidden.shape)  # 1*40*30 num_layers(RNN包含隐藏层数不设置就是1) * batch_size(每次训练样本个数) * char_dim  在最后一个时间步的隐藏状态， 它携带了序列的“历史记忆”
# 去掉值为中间文本长度的维度 提取每个样本在最后一个时间步的隐藏状态
x = rnn_out[:, -1, :]  # 或者写hidden.squeeze()也是可以的，因为rnn的hidden就是最后一个位置的输出
# print('x', x.shape)  # 40*30 batch_size * char_dim(每个字的维度)
```

##### 17.Embedding， pooling， Linear

```python
# vocab_size： 词表大小， embedding_size：字符向量维度
self.word_vectors = nn.Embedding(vocab_size, embedding_size)
# sentence_length 样本文本长度
self.pool = nn.AvgPool1d(sentence_length)   #池化层
# 映射的值取决于想要什么, 是否有某些特定字符出现, 出现或不出现 所以每次都取1个， 或者要预测下个字是什么就映射为词表的大小
self.classify = nn.Linear(embedding_size, 1)     #线性层
self.classify = nn.Linear(embedding_size, vocab_size)     #线性层

def forward(self, context):
    context_embedding = self.word_vectors(context)  # batch_size * window_length * embedding size  1*4*5
    print('context_embedding', context_embedding.shape)  # torch.Size([1, 4, 5])
    # transpose: batch_size * embedding size * max_length -> pool: batch_size * embedding_size * 1 -> squeeze:batch_size * embeddig_size
    #  pool需要的形状是 (batch_size, embedding size, window_length)
    # context_embedding.transpose(1, 2)) 转职context_embedding的12维， squeeze()去掉 去除维度为 1 的维度
    context_embedding = self.pool(context_embedding.transpose(1, 2)).squeeze()
    # batch_size * embeddig_size -> batch_size * vocab_size
    pred = self.classify(context_embedding)
    return pred


```

##### 18.lambda匿名函数

```python
# lambda 参数列表: 表达式
add = lambda x, y: x + y
print(add(3, 4))  # 输出 7
```

##### 19.log对数

```python
import math
result = math.log(20,10) # 以10为底的20的对数
```

##### 20.保存模型权重， 加载训练好的权重

```python
# 保存模型, 保存模型权重
torch.save(model.state_dict(), "model.path")
# 加载训练好的权重
model.load_state_dict(torch.load(model_path))  # 加载训练好的权重

model = Word2Vec(corpus, vector_size=dim, sg=1)
# 将训练好的模型保存到名为"model.w2v"的文件中， 后缀".w2v"通常用于表示Word2Vec模型文件
# Gensim库提供了自己的方法来保存和加载Word2Vec模型，即Word2Vec.save和Word2Vec.load
model.save("model.w2v")
model = Word2Vec.load(path)
```

## 编程小知识点汇总二

##### 21.数组为空, 每append一次字符串, 数组的长度都会加1

```python
my_list = []
words = ["apple", "banana", "cherry"]

for word in words:
    my_list.append(word)

print(my_list)  # 输出: ['apple', 'banana', 'cherry']
print(len(my_list))  # 输出: 3

```

##### 22.集合（set）数据结构 集合可以自动去重

```python
idf_dict = {
    "apple": {0, 1},
    "banana": {1, 2}
}
idf_dict["apple"].add(3)

idf_dict = {
    "apple": {0, 1, 3},
    "banana": {1, 2}
}

```

##### ==23.排序==

sorted 用于迭代对象（列表、元组、字符串、字典、集合等） 返回新列表, 原列表不变

sort 只能用于列表, 返回 None, 改变原列表

对象.items() **返回值形式**：一个类集合对象，包含 `(key, value)` 元组。

```python
result = [('apple', 3), ('banana', 1), ('cherry', 2)]
# 根据result的元组第二个元素来降序排序
result = sorted(result, reverse=True, key=lambda x: x[1])
result = [('apple', 3), ('cherry', 2), ('banana', 1)]

```

##### 24.view

view和reshape的区别：

都是改变数组形状的函数

修改数据后，view数据同步，reshape数据不同步

##### ==25.jieba.lcut(title) , jieba.cut(title) 和 join和split==

```python
import jieba
title = "人工智能在医疗领域的应用"
# jieba.lcut(title) 返回分词列表
words = jieba.lcut(title)
# ['人工智能', '在', '医疗', '领域', '的', '应用']

# jieba.cut(title) 生成的是一个生成器，generator，也就是可以通过for循环来取里面的每一个词。
words= [word for word in jieba.cut(title)]
print(words) # ['人工智能', '在', '医疗', '领域', '的', '应用']

# " ".join用空格作为分割器将列表中的字符串拼接起来, append只能用于列表
sentences = []
sentences.append(" ".join(jieba.lcut(title)))
print(sentences)  # ['人工智能 在 医疗 领域 的 应用']
words = sentences[0].split()  # sentences列表里只有一个string
print(words)  # ['人工智能', '在', '医疗', '领域', '的', '应用']
# 等价于 sentences列表里有多个string
words = [word for sentence in sentences for word in sentence.split()]
# 等价于
# words = []
# for sentence in sentences:
#     for word in sentence.split():
#         words.append(word)

print(words)
```

==**BertTokenizer** 和**jieba.lcut** 区别和共同点==

BertModel 加载了一个预训练的 BERT 模型(相当于别人帮我训练好了),

BertTokenizer 加载一个预训练的BERT分词器

```python
from transformers import BertTokenizer

# 加载 BERT Tokenizer用于加载预训练模型和分词器
tokenizer = BertTokenizer.from_pretrained("bert-base-chinese")

# 分词和编码
text = "我爱自然语言处理"
encoded_input = tokenizer(text)
print(encoded_input)
# 输出: {'input_ids': [101, 2769, 4263, 4696, 1920, 2110, 102], ...}



import jieba
# 分词
text = "我爱自然语言处理"
words = jieba.lcut(text)
print(words)
# 输出: ['我', '爱', '自然语言', '处理']

# BertTokenizer 适用于深度学习任务，尤其是基于 Transformer 的模型（如 BERT、GPT 等）适合处理需要模型理解的输入格式的任务（如文本分类、文本匹配、问答系统等）
# jieba.lcut适用于传统的中文文本处理任务。适合需要直观分词结果的任务（如关键词提取、文本分析、搜索引擎等）

# 共同点都是都是分词工具
```



##### 26.np.concatenate

```python
# np.concatenate：这是 NumPy 中的一个函数，用于将多个数组沿着指定的轴进行拼接, axis=1 表示在第二个维度（列）上进行拼接
w_i = np.concatenate([w_i_h, w_i_x], axis=1)

w_i_h = np.array([[1, 2, 3, 4],
                  [5, 6, 7, 8],
                  [9, 10, 11, 12]])

w_i_x = np.array([[13, 14, 15, 16],
                  [17, 18, 19, 20],
                  [21, 22, 23, 24]])

w_i = np.array([[ 1,  2,  3,  4, 13, 14, 15, 16],
                [ 5,  6,  7,  8, 17, 18, 19, 20],
                [ 9, 10, 11, 12, 21, 22, 23, 24]])

```

##### 27.f.read()

##### json.loads()

用于将JSON格式的字符串转换为Python数据结构（如字典、列表等）。

##### json.dump()

用于将 Python 对象序列化为 JSON 格式的字符串。

```python
json.dump(results, f, ensure_ascii=False, indent=4)
    # json.dump() 是 json 模块中的一个函数，用于将 Python 对象序列化为 JSON 格式并写入文件
    # results 是要被序列化的 Python 数据结构。
    # f 是文件对象，表示要将 JSON 数据写入的文件。
    # ensure_ascii=False 参数表示在输出 JSON 数据时，非 ASCII 字符将被保留为它们原来的样子，而不是被转义为 Unicode 转义序列。这对于包含中文字符的 JSON 数据特别有用。
    # indent=4 参数表示在输出 JSON 数据时，使用 4 个空格进行缩进，使得输出的 JSON 数据更易于阅读
```

```python
# file.read()用于读取文件的内容。如果文件的内容是文本，file.read()将返回一个包含文件内容的字符串
with open(schema_path, encoding="utf8") as f:    
	#  f.read() 方法读取文件的全部内容 json.loadsJSON 格式的字符串转换为 Python 的字典或列表对象，并返回这个对象    
	return json.loads(f.read())

```

##### **

```python

```

##### 29.数组解包

```python
line = [1, 2]
question, label = line
print(question)  # 输出：1
print(label)     # 输出：2
```

##### 30.Pool

```python
x = nn.MaxPool1d(x.shape[1])(x.transpose(1, 2)).squeeze()"""    初始形状: batch_size, max_length, hidden_size    x.transpose(1, 2) 交换维度: batch_size, hidden_size, max_length 使得池化时max_length在最后一个维度    nn.MaxPool1d(x.shape[1])：定义最大池化层 即max_length ->batch_size, hidden_size, 1    .squeeze()：压缩维度移除所有大小为1的维度 -> batch_size, hidden_size (128, 256)"""
```

##### 31.LSTM

 一种多层按顺序执行的写法，具体的层可以换
    unidirection:batch_size, max_len, hidden_size
    bidirection:batch_size, max_len, hidden_size * 2

nn.LSTM中第一个hidden_size（输入维度，input_size）, 表示输入数据的特征维度。例如，若输入是词向量，该参数对应词向量的长度
第二个hidden_size（隐藏状态维度）, 表示LSTM隐藏状态的维度，即每个时间步输出的特征数。它决定了模型记忆信息的能力，较大的值能捕获更复杂模式，但会增加计算量

bidirectional=True 明确启用双向LSTM的标志 batch_first=True 形状为: batch_size seq_len, hidden_size, batch_first=False, 形状为: seq_len, batch_size, hidden_size
若定义nn.LSTM(256, 256)：

输入：每个时间步的输入形状为(batch_size, 256)。
隐藏状态：每个时间步的输出和隐藏状态形状均为(batch_size, 256)。
双向 LSTM 层 输入形状：(batch_size, seq_len, hidden_size)
双向LSTM的输出维度为 hidden_size * 2(双向), 形状: batch_size seq_len, hidden_size * 2
双向LSTM通过同时处理序列的正向（从左到右）和反向（从右到左）信息，将两个方向的隐藏状态拼接（concatenate）后作为最终输出。其核心特征是输出的特征维度会翻倍
LSTM 的输出结构：

output: 所有时间步的隐藏状态，形状为 (batch_size, seq_len, hidden_size*2)。
h_n: 最后一个时间步的隐藏状态，形状为 (num_layers*2, batch_size, hidden_size)。
c_n: 最后一个时间步的细胞状态，形状同 h_n。

nn.ReLU() 输入/输出形状：保持 (batch_size, seq_len, hidden_size*2) 引入非线性，增强模型表达能力。

nn.Linear输入形状：(batch_size, seq_len, hidden_size*2)。
输出形状：(batch_size, seq_len, hidden_size)。
作用：将双向 LSTM 输出的拼接特征（维度 2*hidden_size）映射回 hidden_size，减少参数量。

```python
lstm = nn.LSTM(hidden_size, hidden_size, bidirectional=True, batch_first=True)
```

##### 32.PyTorch 的损失函数（如 nn.CrossEntropyLoss）和神经网络层（如 nn.Linear）只能处理 Tensor 类型的数据

##### 33.bert模型的加载和创建

```python
pretrain_model_path = r'D:\TTT\NLP算法\bert-base-chinese\bert-base-chinese'
# 加载了一个预训练的 BERT 模型
tokenizer = BertTokenizer.from_pretrained(pretrain_model_path)
# 建立模型
def build_model(vocab(字表大小), char_dim(每个字的维度), pretrain_model_path):
    model = LanguageModel(vocab, char_dim, pretrain_model_path)    
    return model
```

##### 34.loss中logits形状targets形状

| 任务类型          | logits形状 | targets形状 | 损失函数            |
| ----------------- | ---------- | ----------- | ------------------- |
| 二分类（2个选项） | [N,2]      | [N]         | `CrossEntropyLoss`  |
| 多标签分类        | [N,C]      | [N,C]       | `BCEWithLogitsLoss` |
| 回归任务          | [N]        | [N]         | `MSELoss`           |

- **PFT** 主要用于高效微调大语言模型，使其适应新任务，同时减少计算和内存开销。
- **蒸馏** 则是一种模型压缩技术，通过知识迁移将大模型的能力传递给小模型，适合在资源受限的场景中部署。

##### RAG

垂直领域的调整催生RAG
大模型在企业知识问答场景中存在的问题:

1. 私域知识不足
2. 知识更新节奏慢
3. 领域建模成本高
4. 幻觉问题
   5.安全合规问题

##### **Milvus**向量数据库

##### 35.// 和 / 和 % 的用法区别

/ (常规除)
如：

5 / 2 = 2.5

解释：平常除法是什么结果就是什么结果。

//（地板除）
如:

5 // 2 = 2 （5 ÷ 2 = 2.5）

5 // 3 = 1 (5 ÷ 3 = 1.6666666666666667)

解释：地板除，只去除完之后的整数部分。

% （取余数）
如：

5 % 2 = 1 （5 - 2*2 = 1）

4 % 2 = 0 （4 - 2*2 = 0）



##### ==36.DataLoader 作用==

`torch.utils.data` 是 Pytorch 中用于加载和预处理数据的模块。它提供了用于创建数据集和数据加载器的类，以便更轻松地处理大型数据集并在训练过程中使用它们。

- `Dataset`：抽象类，代表了一个数据集。为了使用该类，必须要定义三个方法：初始化方法`(__init__`), `__getitem__`, 以及` __len__`方法。来返回数据集的大小和给定索引处的数据。

`DataLoader`是在`Dataset`的基础上，，它可以将`Dataset`返回的单个样本，按照指定的方式进行批量加载数据（`batch_size`）、数据打乱（`shuffle`）。多线程加速数据读取（`num_workers`）。自定义批处理（通过`collate_fn`参数）。

- **创建数据批次**，指定数据打包输出规则：通过batch_size参数，将Dataset中的单个样本打包成一个个批次（batch）的数据。

collate_fn指定如何从N 张训练集选出一个batch的 N /batch_size张图片。
例如batch_size=32，那么DataLoader每次会从Dataset中取出32个样本组成一个batch。每次迭代，返回的是 一个batch 的数据
**自定义数据采样**，指定数据迭代读取规则：

一般使用自定义的采样器（Sampler），实现对数据的特殊采样方式，比如分层采样（在类别不均衡的数据集中，保证每个batch中各类别的样本比例与原始数据集相似）等。
dataset对象是dataloader的一个参数，通过dataset让dataloader知道训练集一共多少图片，从而知道共跌代多少次。
**数据打乱**：通过shuffle参数设置是否在每个epoch开始时打乱数据顺序，这样可以避免模型在训练时对数据产生特定的依赖，有助于模型学习到更通用的特征，提高模型的泛化能力 。

**多进程加载**：通过num_workers参数设置多进程加载数据，从而加快数据加载速度，尤其是在数据量较大、数据预处理较为复杂的情况下，多进程可以充分利用CPU资源，减少数据加载时间，避免数据加载成为训练过程中的瓶颈 。

```python

```



##### ==37.np.random.choice()!!!== 

**是choice 不是choise!!!, replace=首字母大写**

np.random.choice(a, size=None, replace=True, p=None)

a：候选项, 可以是一个数组、列表或元组。
如果传入的是一个整数，则视为从 0 到 a-1 的整数序列。例如，np.random.choice(10) 实际上是从 [0, 1, 2, ..., 9] 中随机抽样。
size：输出样本的形状, 指定返回样本的大小。如果是整数，则返回一个一维数组；也可以传入元组，返回多维数组。如果不指定 size，则默认返回单个元素。
replace：是否允许重复抽样, 默认为 True，表示有放回抽样。当设置为 False 时，即进行无放回抽样，此时抽取的样本数 size 不能大于候选项中元素的个数，否则会报错。
p：各元素被抽中的概率分布,是一个与候选项等长的一维数组，表示每个元素被选中的概率。
如果指定了 p 参数，则必须确保所有概率的和为 1。
如果不指定，则默认使用均匀分布，也就是说每个元素被选中的概率相同。

用于从给定的a中(一维数组或数字范围)随机抽取元素

```python
import numpy as np

# 示例 1：从数组中随机抽取元素（无放回抽样）
# 定义一个数组
arr = np.array([10, 20, 30, 40, 50])
# 从数组中随机选取 3 个不同的元素
sample = np.random.choice(arr, size=3, replace=False)
print(sample)  # [30 40 20]

arr = (10, 20, 30, 40, 50)
# 从数组中随机选取 3 个不同的元素
sample = np.random.choice(arr, size=3, replace=False)
print(sample)  # [40 50 10]

# 示例 2：从指定范围内抽取数字（有放回抽样）
# 从 0 到 9 中随机抽取 4 个数字，允许重复
sample = np.random.choice(10, size=4, replace=True)
print(sample)  # [7 9 2 3]

# 示例 3：使用概率分布进行抽样
# 定义候选项
labels = ['A', 'B', 'C']
# 指定每个候选项的抽样概率
probabilities = [0.1, 0.3, 0.6]
# 从候选项中随机抽取 5 个样本，允许重复
sample = np.random.choice(labels, size=5, replace=True, p=probabilities)
print(sample)  # ['B' 'B' 'C' 'C' 'C']
```

**注意事项:**
无放回抽样限制： 当 replace=False 时，要求 size 不能大于候选项的长度，否则无法抽取足够多的不同元素。
概率分布参数 p： 必须保证 p 中所有值的总和为 1，否则会引发错误。
输入类型： 参数 a 可以是数组、列表或整数，使用时需注意类型转换的情况。

##### ==38.Linear==

Linear只对最后一层进行变换, 由输入512变为输出512   

```python
self.q_linear = nn.Linear(hidden_size, hidden_size)

....
query = self.q_linear(hidden_state)


# 输入hidden_state: [2, 10, 512]
#                     |   |    |
#                     |   |    └── 每个向量512维
#                     |   └── 每个样本10个向量
#                     └── 2个样本
#
# 应用self.q_linear (nn.Linear(512, 512))后:
#                     |   |    |
#                     |   |    └── 每个向量变换为512维
#                     |   └── 每个样本仍然10个向量
#                     └── 仍然2个样本
#
# 输出query: [2, 10, 512], 
```

##### ==39.model.state_dict()和model.parameters()的区别==

`model.parameters()`返回的是一个生成器，**该生成器中只保存了可学习、可被优化器更新的参数的具体的参数，不会保存参数的名字**, 可通过循环迭代打印参数

```python
#coding:utf8

import torch
import torch.nn as nn
import numpy as np

"""
numpy手动实现模拟一个线性层
"""

#搭建一个2层的神经网络模型
#每层都是线性层
class TorchModel(nn.Module):
    def __init__(self, input_size, hidden_size1, hidden_size2):
        super(TorchModel, self).__init__()
        self.layer1 = nn.Linear(input_size, hidden_size1)
        self.layer2 = nn.Linear(hidden_size1, hidden_size2)

    def forward(self, x):
        x = self.layer1(x)   #shape: (batch_size, input_size) -> (batch_size, hidden_size1)
        print(x.shape)  # torch.Size([2, 5])
        y_pred = self.layer2(x) #shape: (batch_size, hidden_size1) -> (batch_size, hidden_size2)
        print(y_pred.shape)  # torch.Size([2, 2])
        return y_pred

#随便准备一个网络输入
x = np.array([[3.1, 1.3, 1.2],
              [2.1, 1.3, 13]])
#建立torch模型
torch_model = TorchModel(3, 5, 2)
#使用torch模型做预测
torch_x = torch.FloatTensor(x)
y_pred = torch_model(torch_x)
print("torch模型预测结果：", y_pred) #tensor([[-0.9281,  0.4829],[-3.7370, -1.2119]], grad_fn=<AddmmBackward0>)
print(torch_model.state_dict())
# OrderedDict({'layer1.weight': tensor([[ 0.1825,  0.0247,  0.0331],
#         [ 0.3361, -0.4516,  0.2612],
#         [ 0.3090,  0.0556,  0.3910],
#         [ 0.2677,  0.2238,  0.0560],
#         [-0.0463, -0.4994, -0.2432]]), 'layer1.bias': tensor([ 0.0891, -0.1511,  0.2531, -0.4090, -0.0039]), 'layer2.weight': tensor([[-0.0137, -0.0425,  0.0238,  0.0929, -0.4347],
#         [ 0.4195,  0.0297,  0.0053, -0.2492,  0.2820]]), 'layer2.bias': tensor([-0.1086, -0.1928])})
print("torch_model.parameters", type(torch_model.parameters()))  # <class 'generator'>
for param in torch_model.parameters():
	print("param", param)
    # param Parameter containing:
    # tensor([-0.1218,  0.0684, -0.3477, -0.2649,  0.1474], requires_grad=True)
    # param Parameter containing:
    # tensor([[-0.3342,  0.1462,  0.1881, -0.0996,  0.3189],
    #         [ 0.0008,  0.1856,  0.0713, -0.1232,  0.1859]], requires_grad=True)
    # param Parameter containing:
    # tensor([-0.1611, -0.2478], requires_grad=True)
```

`model.state_dict()`返回的是一个有序字典OrderedDict，**该有序字典中保存了模型所有参数的参数名和具体的参数值，所有参数包括可学习参数和不可学习参数，可通过循环迭代打印参数**, 因此，该方法可用于保存模型，当保存模型时，会将不可学习参数也存下，当加载模型时，也会将不可学习参数进行赋值。

```python
# model.state_dict()的用法
    print("dict", model.state_dict())  # OrderedDict类型

    for name, param in model.state_dict().items():
        print("name", name)  # embedding.weight等等...
        print("params", param)  # tensor([[ 5.7507e-02, -1.2952e+00,  9.0006e-01, -1.8664e-02,  1.0803e+00....]])
        print("requires_grad", param.requires_grad)  # False
```

##### ==40.self,和____init____==

`self` 表示类实例本身。当你定义一个类的方法时，第一个参数通常是 `self,`它指向调用该方法的实例。通过 `self`，你可以在类的方法中访问实例的属性和其他方法。

构造方法 ____init____：

__init__ 方法是类的构造方法，当创建一个新的类实例时自动调用。
self 参数表示新创建的实例。
self.name 和 self.age 是实例属性，它们属于每个具体的实例。
实例方法 introduce：

introduce 方法是一个普通的实例方法，用于打印一条介绍信息。
self 参数允许方法访问实例的属性 name 和 age。

```python
class Person:
    def __init__(self, name, age):
        self.name = name  # 实例属性
        self.age = age  # 实例属性

    def introduce(self):
        self.age += 1  # 修改实例属性
        print(f"Hello, my name is {self.name} and I am {self.age} years old.")

    def have_birthday(self, name, age):
        self.age += 1  # 修改实例属性
        print(f"Happy birthday, {name}! You are now {age} years old.")


# 创建 Person 类的实例
person1 = Person("Alice", 30)

# 调用实例方法
person1.introduce()
person1.have_birthday('hh', 18)
```

##### ==41.super().____init____()== 

1.super()的过程其实是调用运行了一次父辈的某个函数。super().后面不仅可以跟__init__()，也可以跟别的函数。super().__init__()的意义是在自己类的初始化过程中运行一次父类的__init__()。

2、即使不写super().__init__()，子类也是自动继承了父类的所有非私有方法。

```python
class A1:
    def __init__(self):
        print("A1 init")

    def func(self):
        print("A1 func")


class A2(A1):
    def __init__(self):
        pass
#这个方法中只有pass语句，没有显式调用父类A1的__init__方法。因此，当创建A2的实例时，只会执行A2的__init__方法，而不会自动执行父类的__init__方法

a2 = A2()
a2.func()

# 运行结果：A1 func
```

当用super().__init__()，就运行了一次父类的__init__()

```python
class A1:
    def __init__(self):
        print("A1 init")
 
class A2:
    def __init__(self):
        print("A2 init")
 
class A3(A1):
    def __init__(self):
        super().__init__()  # 调用A1的__init__()方法
        print("A3 init")
A3()
 
# 运行结果：
A1 init
A3 init
```

##### ==42.np.array([])（NumPy 数组）和普通 Python list（列表）的区别==

1. **NumPy数组的高级索引**：NumPy支持一种称为"高级索引"或"花式索引"(fancy indexing)的功能，它允许你使用另一个数组（或列表）作为索引来获取原数组中的多个元素。
2. **索引过程**：
   - NumPy将列表`a = [0, 1, 2, 3]`视为索引集合
   - 对于`a`中的每个索引值，NumPy从数组`x`中取出对应位置的元素
   - 具体来说：
     - 取出`x[0]`，即`'a'`
     - 取出`x[1]`，即`'b'`
     - 取出`x[2]`，即`'c'`
     - 取出`x[3]`，即`'d'`
3. **结果**：执行`b = x[a]`后，`b`将是一个新的NumPy数组，包含`['a', 'b', 'c', 'd']`。

```python
import numpy as np
index = [0, 1, 2, 3]
x = np.array(['a', 'b', 'c', 'd'])
z = x[index]
print(z) # ['a' 'b' 'c' 'd']
```

普通列表需要

```python
index = [0, 1, 2, 3]
x = ['a', 'b', 'c', 'd']

# 使用列表推导式
z = [x[i] for i in index]
print(z)  # 输出: ['a', 'b', 'c', 'd']
```

普通列表转为np.array

```python
import numpy as np

list = [1, 2, 3]
new_list = np.array(list)
print(type(new_list))  # <class 'numpy.ndarray'>
print(new_list)  # [1 2 3]
# NumPy 数组 np.array元素之间没有逗号，只用空格分隔。
```



##### ==43.for循环==

```python
# 1. 遍历列表
fruits = ['apple', 'banana', 'cherry']
for fruit in fruits:
    print(fruit)
    # apple
    # banana
    # cherry

# 2. 遍历元组
colors = ('red', 'green', 'blue')
for color in colors:
    print(color)
    # red
    # green
    # blue

# 3. 遍历字符串
for char in "Python":
    print(char)
    # P
    # y
    # t
    # h
    # o
    # n
# 4. 遍历集合
unique_numbers = {1, 2, 3, 2, 1}
for num in unique_numbers:
    print(num)
    # 1
    # 2
    # 3
# 5. 遍历字典
person = {'name': 'Alice', 'age': 25, 'city': 'New York'}
# 遍历键
for key in person:
    print(key)
    # name
    # age
    # city
# 遍历值
for value in person.values():
    print(value)
    # Alice
    # 25
    # New York
# 遍历键值对
for key, value in person.items():
    print(f"{key}: {value}")
    # name: Alice
    # age: 25
    # city: New York

# 6. 遍历range(数字!!!)
for i in range(5):
    print(i)
    # 0
    # 1
    # 2
    # 3
    # 4
# 7. 遍历文件
file_path = r'category_corpus/auto.txt'
with open(file_path, 'r', encoding='utf-8') as f:
    # 每个字符占一行，输出格式完全改变
    for line in f.read():
        print(line)
        # 世
        # 界
        # ，
        # 尽s
        # 在
        # 于
        # 心
        # ：
with open(file_path, 'r', encoding='utf-8') as f:
    # 按行输出，保持原有格式
    for line in f:
        print(line)
        # 世界，尽在于心：全新梅赛德斯-奔驰S级轿车今日上市\n

# 8. 遍历enumerate
fruits = ['apple', 'banana', 'cherry']
for index, fruit in enumerate(fruits):
    print(f"Index {index}: {fruit}")
    # Index 0: apple
    # Index 1: banana
    # Index 2: cherry

# 9. 遍历zip
names = ['Alice', 'Bob', 'Charlie']
ages = [25, 30, 35]
for name, age in zip(names, ages):
    print(f"{name} is {age} years old")
    # Alice is 25 years old
    # Bob is 30 years old
    # Charlie is 35 years old
```

##### ==44.from sklearn.datasets import make_blobs==

make_blobs函数从sklearn.datasets模块生成合成的聚类数据集

```python
from sklearn.datasets import make_blobs
# make_blobs(n_samples=100, n_features=2, centers=None, cluster_std=1.0, center_box=(-10.0, 10.0),shuffle=True, random_state=None)
# n_samples指定生成的数据点总数, n_features：默认值= 2,每个样本的特征数量。centers指定要生成的中心点（簇）的数量, cluster_std定每个簇的标准差, random_state = 0 这个参数确保每次运行代码时生成的数据集是相同的
#  X 类型：NumPy数组，形状为(n_samples, n_features) (300, 2)
# y_true 类型：NumPy数组，形状为(n_samples,) 含义：每个数据点所属的真实簇标签
X, y_true = make_blobs(n_samples=300, centers=3, cluster_std=0.70, random_state=0)
# print('X', X)  # [[ 0.33729452  5.08569873] [ 1.54734936 -0.07069111] [ 1.50899649  4.38895984]...]
# print(X.shape)  # (300, 2)
# print(y_true.shape)  # (300,)
# print('y_true', y_true)  #  [0 1 0 2 2 2 1 0 2 2 1 1 ...]
```

##### ==45.np.argmin==

找出数组中最小元素的位置, 接收数组, 返回数字

```python
import numpy as np

list = [1, 2, 3]
print(np.argmin(list))  # 0
```

##### ==46.import matplotlib.pyplot as plt==

| 函数名称 | 描述                   |
| -------- | ---------------------- |
| scatter  | 绘制x与y的散点图       |
| plot     | 在坐标轴上画线或者标记 |

```python
# 可视化原始数据
# 创建一个新的图形窗口（figure）figsize：一个元组，指定图形的宽度和高度（单位是英寸） (15, 5)：宽度15英寸，高度5英寸
plt.figure(figsize=(15, 5))
# 在图形窗口中创建一个子图（subplot）
# 第一个参数1：行数，表示图形窗口有1行子图
# 第二个参数3：列数，表示图形窗口有3列子图
# 第三个参数1：当前子图的索引（从1开始）
plt.subplot(1, 3, 1)
# 在当前子图上绘制散点图
# X[:, 0]：数据点的x坐标
# X是一个二维数组，X[:, 0]表示取所有行的第0列（即第一个特征）
# X[:, 1]：数据点的y坐标
# X[:, 1]表示取所有行的第1列（即第二个特征）
# s=50：点的大小（marker size）
# 数值越大，点越大
# 默认值通常是20，这里设置为50使点更加明显
plt.scatter(X[:, 0], X[:, 1], s=50)

# 设置中文字体
plt.rcParams['font.sans-serif'] = ['SimHei']  # 'SimHei'是黑体
plt.rcParams['axes.unicode_minus'] = False  # 解决负号显示问题
plt.title("原始数据")

# 使用我们的K-Means算法聚类
kmeans = KMeans(k=3, max_iters=100, tol=1e-4)
kmeans.fit(X)

# 获取聚类结果
labels = kmeans.predict(X)
centroids = kmeans.centroids
# print('labels', labels)  #labels [2 0 2 1 1 1 0 2 1 2 0 0 0 2...]

# 可视化聚类结果
plt.subplot(1, 3, 2)
# c=labels 是一个关键参数，它指定每个点的颜色
# cmap='viridis'：颜色映射（colormap），用于将数字标签转换为颜色
plt.scatter(X[:, 0], X[:, 1], c=labels, s=50, cmap='viridis')
# c=labels：点的颜色，根据每个数据点的簇标签决定
# marker='X' 设置点的标记样式：
plt.scatter(centroids[:, 0], centroids[:, 1], c='red', s=200, alpha=0.75, marker='X')
plt.title("我们的K-Means结果")
plt.show()


# Plot	在坐标轴上画线或者标记
# 参数1. x轴(列表), 参数2y轴(列表), 参数3是标签
plt.plot(range(len(log)), [l[0] for l in log], label="acc")  # 画acc曲线
# 添加图例
plt.legend()
```

##### ==47.日志记录logging==

| 序号 | 级别     | 级别数值 | 使用情况                                                     |
| ---- | -------- | -------- | ------------------------------------------------------------ |
| 1    | NOTEST   | /        | 不记录任何日志信息                                           |
| 2    | DEBUG    | 10       | 用于记录开发过程中的细节信息，例如函数调用，变量值等         |
| 3    | INFO     | 20       | 用于记录程序正常运行过程中的一般信息                         |
| 4    | WARNING  | 30       | 用于记录可能导致问题的潜在问题，例如语法警告、网络连接中断等 |
| 5    | ERROR    | 40       | 用于记录程序运行过程中发生的错误，例如函数调用失败，异常发生等 |
| 6    | CRITICAL | 50       | 用于记录严重的错误，例如程序奔溃等                           |

级别从低到高依次为: NOTEST < DEBUG < INFO < WARNING < ERROR < CRITICAL, 默认为WARNING级别, 默认情况下日志打印只显示大于等于 WARNING 级别的日志

**1. logging.basicConfig**

通过`logging.basicConfig`函数对日志的输出格式及方式做相关配置

**2.记录器Logger**

Logger 持有日志记录器的方法，日志记录器不直接实例化，而是通过模块级函数`logging.getlogger (name)`来实例化

```python
import logging
# level: 指定打印日志的级别,debug,info,warning,error,critical
# format: 日志输出相关格式
# %(asctime)s：日志记录的时间
# %(name)s：记录器的名称（通常是模块名）
# %(levelname)s：日志级别名称（如 INFO、WARNING 等）
# %(message)s：实际的日志消息内容
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)
# 2025-09-14 19:53:21,115 - __main__ - INFO - epoch 1 begin
# 2025-09-14 19:53:22,818 - __main__ - INFO - batch loss 3.219985
```

##### ==49.seed随机种子==

seed() 函数用于设置随机数生成器的种子，在相同的种子下，每次生成的随机数序列都是一样的。因为数据重复可重复可复现

```python
# 定义随机种子, 同样数据同样模型可以复现
# 从配置对象 Config 中获取一个名为 seed 的值，并将其赋给变量 seed。这个值通常是一个整数，用于初始化随机数生成器
seed = Config["seed"]
# 设置随机数生成器的种子。这样，每次运行程序时，random 模块生成的随机数序列都是相同的
random.seed(seed)  # Python 中的 random 模块, 用于生成伪随机数序列。以确保每次程序运行时都产生相同的随机数序列
np.random.seed(seed)  # NumPy 中的 numpy.random, 用于生成 NumPy 库中的随机数组。
torch.manual_seed(seed)  # 机器学习库中的随机性控制：比如 PyTorch 中的 torch.manual_seed(seed) 用于控制随机数生成，在模型训练中确保重复性。
torch.cuda.manual_seed_all(seed)  # 深度学习框架中的 GPU 随机性控制：对于使用 GPU 的深度学习框架（如 PyTorch、TensorFlow），通常也会有类似 torch.cuda.manual_seed(seed) 的函数，用于设置 GPU 相关的随机种子，以保证实验结果的一致性。
for i in range(3):
    random_number = random.random()
    print('random_number_{}:{}'.format(i, random_number))
    # 每次执行文件都会生成同样的数据
    # random_number_0:0.5327547554445934
    # random_number_1:0.5288157818367566
    # random_number_2:0.6265608260743084
random_tensor_cpu = torch.rand(3)
print("Random Tensor (CPU):", random_tensor_cpu)
# tensor([0.9281, 0.4066, 0.4132])
if torch.cuda.is_available():
    # 在GPU上生成随机张量
    for i in range(3):
        random_tensor_gpu = torch.rand(3).cuda()
        print("Random Tensor (GPU):", random_tensor_gpu)  # 一维数组
```

##### ==50.序列==

**序列**是一种有序的集合数据类型，它可以存储任意类型的对象。序列中的每个元素都分配有一个从0开始的数字索引，可以通过此索引访问特定位置的元素。

| 序列类型             | 可变性 | 描述                             |
| -------------------- | ------ | -------------------------------- |
| 列表 (list)          | 可变   | 由方括号 [] 包围，元素用逗号分隔 |
| 元组 (tuple)         | 不可变 | 由圆括号 () 包围，元素用逗号分隔 |
| 字符串 (str)         | 不可变 | 由引号包围的字符序列             |
| 字节序列 (bytes)     | 不可变 | 包含单个字节的不可变序列         |
| 字节数组 (bytearray) | 可变   | 可变的字节序列                   |
| 范围 (range)         | 不可变 | 表示一个不可变的数字序列         |

所有序列类型（无论可变还是不可变）都支持以下操作

1. 索引 (Indexing)
   使用方括号 [] 访问序列中的元素：

```python
sequence[index]  # 索引从0开始
```

2. 切片 (Slicing)

   获取序列的子序列：

```python
sequence[start:end:step]  # 从start到end-1，步长为step
```

3. 连接 (Concatenation)

​	使用 `+` 运算符连接相同类型的序列：

```python
sequence1 + sequence2
```

4. 重复 (Repetition)

使用 `*` 运算符重复序列：

```python
sequence * n  # 重复n次
```

5. 成员测试 (Membership)

使用 `in` 和 `not in` 运算符：

```python
element in sequence
element not in sequence
```

6. 迭代 (Iteration)

使用 `for` 循环遍历序列中的元素：

```python
for element in sequence:
    # 处理元素
```

7. 长度、最小值和最大值

```python
len(sequence)  # 序列长度
min(sequence)  # 序列中的最小元素
max(sequence)  # 序列中的最大元素
```

```python
# 列表
sequence = ['a', 'b', 'c']
sequence1 = ['cherry']
sequence2 = ['apple', 'bannana']
print(sequence[2])  # c
print(sequence[0:2:1])  # ['a', 'b']
print(sequence1 + sequence2)  # ['cherry', 'apple', 'bannana']

print(sequence * 2)  # ['a', 'b', 'c', 'a', 'b', 'c']

print('a' in sequence)  # True
print('a' not in sequence)  # False
for element in sequence:
    print(element)
    # a
    # b
    # c
print(len(sequence))  # 3
print(min(sequence))  # a # 首字母比较
print(max(sequence))  # c

# 元组
sequence = (1, 2, 3, 4, 5)
sequence1 = ('a', 'b')
sequence2 = ('uy', 'po')
print(sequence[2])  # 3
print(sequence[0:2:1])  # (1, 2)
print(sequence1 + sequence2)  # ('a', 'b', 'uy', 'po')

print(sequence * 2)  # (1, 2, 3, 4, 5, 1, 2, 3, 4, 5)

print('a' in sequence)  # False
print('a' not in sequence)  # True
for element in sequence:
    print(element)
    # 1
    # 2
    # 3
    # 4
    # 5
print(len(sequence))  # 5
print(min(sequence))  # 1
print(max(sequence))  # 5

# 字符串
sequence = 'apple'
sequence1 = 'banana'
sequence2 = 'cherry'
print(sequence[2])  # p
print(sequence[0:4:1])  # appl
print(sequence1 + sequence2)  # bananacherry

print(sequence * 2)  # appleapple

print('a' in sequence)  # True
print('a' not in sequence)  # False
for element in sequence:
    print(element)
    # a
    # p
    # p
    # l
    # e
print(len(sequence))  # 5
print(min(sequence))  # a
print(max(sequence))  # p
```

**字典和集合**

| 序列类型   | 可变性 | 描述          |
| ---------- | ------ | ------------- |
| 字典(dict) | 可变   | 键值对        |
| 集合 (set) | 不可变 | 去重/集合运算 |

字典长度

```python
# 创建一个字典
my_dict = {
    "name": "Alice",
    "age": 25,
    "city": "New York"
}

# 获取字典的长度
dict_length = len(my_dict)

# 输出字典的长度
print(f"字典的长度是: {dict_length}")  # 字典的长度是: 3
```

##### ==51.model.cuda()==

model.cuda() 的核心功能

1. 模型参数和缓冲区迁移
   PyTorch 模型的参数（如权重和偏置）和缓冲区（例如 BatchNorm 的均值与方差）默认存储在 CPU 内存中。
   调用 model.cuda() 会将这些数据转移到 GPU，使得模型的所有计算都在 GPU 上进行。
2. 启用 GPU 加速
   如果模型未转移到 GPU，即便输入数据已经在 GPU 上，计算也会强制回退到 CPU，从而导致性能瓶颈。
   使用 model.cuda()，可以确保模型和输入数据一致位于 GPU 上，充分释放 GPU 的计算潜力。
3. 与多 GPU 支持结合
   model.cuda() 可与 PyTorch 的 torch.nn.DataParallel 或 torch.nn.parallel.DistributedDataParallel 配合，将模型分布到多个 GPU 上，进一步提升训练速度。

```python
# 标识是否使用gpu
cuda_flag = torch.cuda.is_available()
if cuda_flag:
    logger.info("gpu可以使用，迁移模型至gpu")
    model = model.cuda()
```

```python
for index, batch_data in enumerate(train_data):
   optimizer.zero_grad()
   if cuda_flag:
      batch_data = [d.cuda() for d in batch_data]
```

##### ==52.print(f““)==

字符串格式化的语法，称为f-string。它允许在字符串中使用花括号{}来引用Python中的变量或表达式，并将它们的值插入到字符串中

```python
name = "Tom"
age = 18
print(f"My name is {name}, and I am {age} years old.")
# My name is Tom, and I am 18 years old.
```

##### ==53.逻辑运算符 (and / or / not)==

| 运算符 | 逻辑表达式 | 简要说明                                                     |
| ------ | ---------- | ------------------------------------------------------------ |
| and    | x and y    | 布尔"与" - 若 x 为 False，x and y 返回 False，否则返回 y 的计算值 |
| or     | x or y     | 布尔"或" - 若 x 为非 0，则返回 x 的值，否则返回 y 的计算值   |
| not    | not x      | 布尔"非" - 若 x 为 True，返回 False；若 x 为 False，返回 True |
