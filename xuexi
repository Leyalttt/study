【NLP面试题分享学习】
1.NLP有哪些常见任务？

2.请介绍你知道的文本表征的方法(词向量)week5

3如何生成句向量？

4如何计算文本似度
Jaccard相似度: 将文本分词后, 衡量文本相似度, 样本交集 / 样本并集, 0不相似, 1完全相似 
    应用: 1.文本相似性分析, 对比两篇文章的关键词相似性
             2.推荐系统, 用户购买历史相似推荐商品
            3.数据去重, 检测数据集的重复, 删除重复网站和文档 查重去重
5样本不平衡的解决方法？

6过拟合有哪些表现，怎么解决？

7.用过 jiaba 分词吗，了解原理吗

8.了解命名实体识别吗？通常用什么方法，各自有什么特点

9.知道Bert么，讲一下原理？有用过吗？

10.了解RNN吗，LSTM呢，LSTM相对RNN有什么特点

11.会用正则表达式吗？

12.elmo、GPT、bert三者之间有什么区别？

13.假设现在给你一篇报道疫情的文章，里面有国家名称以及对应的确诊人数，你怎么样从这一堆内容里选出国家名称以及对应的确诊人数，并最终给出确诊人数最多的国家

14、re.match() 和 re.search() 有什么区别？
匹配位置:
re.match: 因从字符串开始位置匹配
re.search: 在整个字符串中查找第一个匹配项, 第一项没匹配到返回None
15、word2vec和fastText对比有什么区别, 没匹配到返回None

16.LSTM时间复杂度，Transformer 时间复杂度
单层LSTM时间复杂度 O(T * d * h)
双向LSTM时间复杂度 O(2 *T * d * h)
多层LSTM时间复杂度 O(L * T * d * h)
L表示LSTM网络层的层数
T表示输入序列的长度, 即序列中时间步的数量
d表示输入特征的维度
h表示隐藏层的大小

ransformer 时间复杂度
1. 自注意力机制的时间复杂度
步骤 1：生成 Q, K, V 矩阵
输入序列长度为 n，每个 token 的维度为 d
 Query（Q）、Key（K）、Value（V）
当计算 Q=XWQ 时：
• XX 是 n×d
• WQWQ 是 d×d
• 矩阵乘法的计算量为：n×d×d=nd²
每个矩阵的维度为 n×d 时间复杂度 O(nd²)，共 3×O(nd²)=O(nd²)
步骤 2：计算注意力分数
• QKT 的维度为 n×n
• 时间复杂度：矩阵乘法 Q×KT需要 O(n²d)
步骤 3：加权求和
将注意力权重与 V 相乘 时间复杂度：矩阵乘法 Attention×VAttention×V 需要 O(n²d)
总时间复杂度
自注意力机制的总时间复杂度为：O(n²d)+O(nd²)
2. 前馈网络（FFN）的时间复杂度
• 前馈网络由两个线性层组成，通常中间维度扩展为 4d
• 时间复杂度：
    ◦ 第一层：O(n⋅d⋅4d)=O(nd²)
    ◦ 第二层：O(n⋅4d⋅d)=O(nd²)
• 总时间复杂度为 O(nd²)
3. 单层 Transformer 的时间复杂度
单层 Transformer 包含一个自注意力模块和一个前馈网络：
单层复杂度=O(n2d)+O(nd2)+O(nd2)=O(n2d+nd2) 
4. 整个 Transformer 模型的时间复杂度
假设模型有 LL 层，总时间复杂度为：O(Ln2d+Lnd2)

17、发生过拟合时采取什么操作
过拟合: 训练集上很好, 测试集上很差
1. Dropout
2. 正则化 
18、为什么Dropout和正则化可以防止过拟合

19、如果要你对问答库的问题相似度匹配先进行一个粗选，再进行细致的选择，以加快匹配速度，有什么思路

21.知识提取提取得到的具体内容是什么？

22.知识图谱是如何构建的？

24.知识图谱的项目背景是什么，为什么要做百科知识图谱；数据怎么获取的，标注了多少数据，数据的实体识别数据格式是怎么样的

25.为什么要用开放式关系抽取，开放式关系抽取的训练数据怎么标注的

26.实体有哪些类型，关系有哪些类型，开放式关系抽取的关系类型目前有几种，怎么对开放式关系抽取抽取出来的关系进行归一化，还是抽出来哪些就是哪些

27.模型选择为什么选用lstm+crf，现在应该有很多新的sota模型，为什么不用，有做过效果对比吗

30..LSTM模型的原理是什么

31.Crf模型的原理是什么

32.Bert中的多头注意力原理是什么

34.已经训练好一个NER模型后如何在这个模型的基础上增加新的标签？

36.大概说一下情感分析的业务内容

37.词向量是自己训练的么，用的哪个工具，调用的哪个包

38.经过LSTM之后，接池化的操作方式（可能涉及到textCNN）

39.简单说一下使用fasttext的理由，fattext用的是哪个包

40.怎么解决数据不均衡的问题

41.那为什么要用LSTM呢

42.说一下transformer

43.为什么Bert会大火

44.简单介绍一下KNN，了解KNN的优化算法KD树么

45.self-attention了解多少，它和attention的区别在哪

46.为什么要自己训练词向量，不采用预训练好的词向量

47.你了解预训练语言模型吗？（bert，ERNIE，XLNet，ALBert，aliceche）

48.Bert中抽出15%的词进行mask，然后80%mask，10%不变，10%任意字符替换，为什么不选用全部mask

49.情感分析的时候，聚类算法的输入是什么

50.如何在直播的问答场景中增加拟人化的回答，像真实的主播一样

51.bert的mask有什么策略  

52.kmeans聚类和knn聚类的区别

53.kmeans聚类初始的那个圈的大小怎么确定的

54.svm有什么缺点，怎么解决的，常用核函数有什么

60.聚类算法都有啥

61.文本匹配和文本分类有什么区别

62.智能问答的损失函数用啥，平时调参都调什么参数，batch size对训练有影响吗，有什么影响

63.bert预训练是做什么训练

64.GPT的原理和bert的区别

65.思维链和思维树的区别，思维树的每个节点是等可能的吗，会有无限的路径吗，思维树的结果是怎么得出来的

66.已经训练好一个NER模型后如何在这个模型的基础上增加新的标签？

67.self attention的这一步的时间复杂度是多少

68.从sgd到adam的两种演进方向是什么，一种是adamw，具体做了什么，有什么改进

69.self attention中，qk除以根号下dk这一步，可以理解为将q，k两个mean为0，var为1的矩阵的乘积qk的方差归一化，为什么q，k是mean为0，var为1的矩阵

70.亿级数据要训练，你怎么把他们导入作训练，这个要怎么答呢？

71.数据的实体识别数据格式是怎么样的

72.思维链和思维树的区别，思维树的每个节点是等可能的吗，会有无限的路径吗，思维树的结果是怎么得出来的

73.如果要你对问答库的问题相似度匹配先进行一个粗选，再进行细致的选择，以加快匹配速度，有什么思路

74.传统的机器学习算法了解哪些

75.Kmeans的K如何选取

76.Kmeans 的评价指标是什么?类内距离和类间距离有什么

77.梯度下降的SGD是怎么做的?

78.了解Moment梯度下降是怎么做的?

79.详细描述Adam优化器(每一步!!)

80.Adam的一阶动量相比于SGD有什么优势?

81.导数和梯度的区别?

82.激活函数有哪些?

83.gelu和relu的区别?

84.sigmoid的作用?

85.sigmoid放在中间层会发生什么?11.bert的自注意力是怎么实现的?

86.自注意力的QK乘完后需要归一化吗?

87.bert适用于什么场景?14. chatGPT适合什么任务?
